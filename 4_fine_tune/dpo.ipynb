{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "\n",
    "import datasets # type: ignore\n",
    "\n",
    "# tmds = datasets.load_dataset(\"ZSvedic/gpt4o-arena-brevity-dpo\")\n",
    "# dataset = tmds['train']\n",
    "# eval_dataset = tmds['test']\n",
    "dataset = eval_dataset =datasets.load_dataset('jondurbin/truthy-dpo-v0.1', split='train')\n",
    "# dataset = datasets.load_dataset('ZSvedic/phi3-arena-short-dpo', split='train')\n",
    "# eval_dataset = datasets.load_dataset('ZSvedic/phi3-arena-short-dpo', split='test')\n",
    "print(f\"Train dataset: {len(dataset)}, Test dataset: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and corresponding tokenizer.\n",
    "import torch # type: ignore\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')  # Add the parent directory to the Python path\n",
    "import utils.llm_utils as llm\n",
    "\n",
    "# model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model_name = 'microsoft/phi-2'\n",
    "# model_name = 'Manoj21k/microsoft-phi-2-finetuned'\n",
    "tokenizer, model = llm.load_tokenizer_and_model(model_name)\n",
    "# From: https://github.com/jndiogo/LLM-chat-templates?tab=readme-ov-file#phi-2\n",
    "# tokenizer.chat_template = \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'].strip() + '\\n\\n' %}{% else %}{% set loop_messages = messages %}{% set system_message = '' %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ 'Instruct: ' + content.strip() + '\\n' }}{% elif message['role'] == 'assistant' %}{{ 'Output: '  + content.strip() + '\\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Output:' }}{% endif %}\"\n",
    "\n",
    "# I don't think this is needed, but everbody sets ref_model.\n",
    "from transformers import AutoModelForCausalLM \n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\", \n",
    "        trust_remote_code=True,\n",
    "        # Required for packing: https://huggingface.co/blog/packing-with-FA2\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "        )\n",
    "\n",
    "print(f'Allocated GPU memory: {torch.cuda.memory_allocated() / (1024*1024):,.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZEL: Test the model verbosity before fine-tuning.\n",
    "\n",
    "def calc_model_avg_len(tokenizer, model):\n",
    "\n",
    "    questions = [\n",
    "        \"How much is 2+3?\",\n",
    "        \"What is the color of the sky?\",\n",
    "        \"What is the capital of France?\",\n",
    "        \"What is the boiling point of water?\",\n",
    "        \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "        \"What is the largest planet in our solar system?\",\n",
    "        \"What is the chemical symbol for gold?\",\n",
    "        \"How many continents are there?\",\n",
    "        \"What is the speed of light?\",\n",
    "        \"Who painted the Mona Lisa?\",\n",
    "        \"What is the smallest prime number?\",\n",
    "        \"What is the main ingredient in guacamole?\",\n",
    "        \"What is the square root of 64?\",\n",
    "        \"What is the currency of Japan?\",\n",
    "        \"Who discovered penicillin?\",\n",
    "        \"What is the tallest mountain in the world?\",\n",
    "        \"What is the primary language spoken in Brazil?\",\n",
    "        \"What is the freezing point of water?\",\n",
    "        \"What is the largest mammal?\",\n",
    "        \"What is the capital of Japan?\"\n",
    "    ]\n",
    "\n",
    "    # messages = [ [{\"role\": \"user\", \"content\": q}] for q in questions]\n",
    "    # prompts = [tokenizer.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
    "    #         for m in messages]\n",
    "    prompts = [f\"Instruct: {q}\\nOutput: \" for q in questions]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding=True).to('cuda')\n",
    "    inputs_tok_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    results = model.generate(**inputs, max_new_tokens = 200, use_cache = True)\n",
    "    sequences = tokenizer.batch_decode(results[:, inputs_tok_len:], skip_special_tokens=True)\n",
    "\n",
    "    averages = []\n",
    "    for p, answer in zip(prompts, sequences):\n",
    "        print(f\"PROMPT: {p}\\nANSWER: {answer}\")\n",
    "        print('------------------------------------------------------------------------------------')\n",
    "        averages.append(len(answer))\n",
    "\n",
    "    total_average = sum(averages)/len(averages)\n",
    "    print(f\"Average {model.name_or_path} answer length for {len(averages)} questions: {total_average:.2f} characters\")\n",
    "\n",
    "    return total_average\n",
    "\n",
    "# Then run the verbosity test:\n",
    "model.eval()\n",
    "avg_len_before = calc_model_avg_len(tokenizer, model)\n",
    "model.train()\n",
    "print(avg_len_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZEL: define the len_metrics function.\n",
    "\n",
    "def len_metrics(pred):\n",
    "    model.eval()\n",
    "    avg_len = calc_model_avg_len(tokenizer, model)\n",
    "    model.train()\n",
    "    return {\"avg_len\": \"avg_len\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "ref_model.eval()\n",
    "\n",
    "# Configure DPOTrainer.\n",
    "\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "# Set up the training arguments\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"../results\",\n",
    "    logging_dir=\"../logs\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    max_prompt_length = 108, # 60 words x 6 ch x 1.2 buffer / 4 chars_in_token\n",
    "    max_length = 324, # (60+120) words x 6 ch x 1.2 buffer / 4 chars_in_token\n",
    "    remove_unused_columns=False,\n",
    "    learning_rate=1e-6,\n",
    "    # gradient_accumulation_steps=8,\n",
    "    # gradient_checkpointing=True,\n",
    "    logging_steps=1,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model, # Not sure if this is needed?\n",
    "    beta=0.1,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_args,\n",
    "    compute_metrics=len_metrics,\n",
    "    )\n",
    "\n",
    "# For debugging purposes, save the initial model before training.\n",
    "trainer.save_model(\"../results/base-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZEL: Run the verbosity test after fine-tuning:\n",
    "model.eval()\n",
    "avg_len_after = calc_model_avg_len(tokenizer, model)\n",
    "\n",
    "print(f\"AVG length before was {avg_len_before}, after fine-tuning it is {avg_len_after}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save.\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
