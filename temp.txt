--------------------------- Testing speed:
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=16_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 66.76 sec
Chunk size: 100, time: 66.76 sec, time/sample: 0.67 sec
Processing chunk of 20 questions...
...total time: 15.72 sec
Chunk size: 20, time: 15.72 sec, time/sample: 0.79 sec
Creating json from Arrow format: 100%|██████████████████████████████████████████████| 1/1 [00:00<00:00, 159.72ba/s]
zel@MLBOX:~/ml-projects/ShortGPT$ 
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=32_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 77.98 sec
Chunk size: 100, time: 77.98 sec, time/sample: 0.78 sec
Processing chunk of 20 questions...
...total time: 27.04 sec
Chunk size: 20, time: 27.04 sec, time/sample: 1.35 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=24_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 82.94 sec
Chunk size: 100, time: 82.94 sec, time/sample: 0.83 sec
Processing chunk of 20 questions...
...total time: 46.93 sec
Chunk size: 20, time: 46.93 sec, time/sample: 2.35 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=20_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 63.62 sec
Chunk size: 100, time: 63.62 sec, time/sample: 0.64 sec
Processing chunk of 20 questions...
...total time: 16.05 sec
Chunk size: 20, time: 16.05 sec, time/sample: 0.80 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=18_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 64.46 sec
Chunk size: 100, time: 64.46 sec, time/sample: 0.64 sec
Processing chunk of 20 questions...
...total time: 15.91 sec
Chunk size: 20, time: 15.91 sec, time/sample: 0.80 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 63.32 sec
Chunk size: 100, time: 63.32 sec, time/sample: 0.63 sec
Processing chunk of 20 questions...
...total time: 15.37 sec
Chunk size: 20, time: 15.37 sec, time/sample: 0.77 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=50,
                          n_chunk_max_ch=22_000):
Never completed!
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=25,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 77.35 sec
Chunk size: 100, time: 77.35 sec, time/sample: 0.77 sec
Processing chunk of 20 questions...
...total time: 17.07 sec
Chunk size: 20, time: 17.07 sec, time/sample: 0.85 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=80, 
                          q_chunk_small=16,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 80 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 70.38 sec
Chunk size: 80, time: 70.38 sec, time/sample: 0.88 sec
Processing chunk of 40 questions...
...total time: 42.50 sec
Chunk size: 40, time: 42.50 sec, time/sample: 1.06 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=150, 
                          q_chunk_small=15,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 120 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 92.68 sec
Chunk size: 120, time: 92.68 sec, time/sample: 0.77 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=10,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 107.36 sec
Chunk size: 100, time: 107.36 sec, time/sample: 1.07 sec
Processing chunk of 20 questions...
...total time: 22.74 sec
Chunk size: 20, time: 22.74 sec, time/sample: 1.14 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=200, 
                          q_chunk_small=20,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 120 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 82.67 sec
Chunk size: 120, time: 82.67 sec, time/sample: 0.69 sec
---------------------------
def smart_chunk_generator(llm_call, 
                          questions, 
                          q_chunk_big=100, 
                          q_chunk_small=20,
                          n_chunk_max_ch=22_000):
Allocated GPU memory: 7,288.4 MB
Processing chunk of 100 questions...
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
...total time: 66.79 sec
Chunk size: 100, time: 66.79 sec, time/sample: 0.67 sec
Processing chunk of 20 questions...
...total time: 16.95 sec
Chunk size: 20, time: 16.95 sec, time/sample: 0.85 sec
---------------------------


---------------------------
