{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88fca5415604736b26c05efbe0073e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Generator\n",
    "import utils.llm_utils as llm\n",
    "import datasets # type: ignore\n",
    "\n",
    "input_dataset = 'lmsys/chatbot_arena_conversations'\n",
    "dataset = datasets.load_dataset(input_dataset)['train']\n",
    "n_rows = len(dataset)\n",
    "print(str(n_rows))\n",
    "\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer, model = llm.load_tokenizer_and_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "out_jsonl = \"1_dataset_creator/phi3_arena_brief_dataset.jsonl\"\n",
    "with open(out_jsonl, 'w') as f: pass\n",
    "\n",
    "dataset = dataset.select_columns(['question_id', 'conversation_a']) \n",
    "dataset = dataset.map(\n",
    "        lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "for normal_chunk in llm.process_variable_chunks(dataset, tokenizer, model, 100, 1500, 50):\n",
    "    normal_chunk = normal_chunk.select_columns(['question_id', 'question', 'answer'])\n",
    "    with open(out_jsonl, 'ab') as f:\n",
    "        normal_chunk.to_json(f, lines=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing first 100...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fecc7de8814645ab22e30185d76e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...total time: 53.63 sec\n",
      "Time/sample: 0.54 sec\n"
     ]
    }
   ],
   "source": [
    "import utils.utils as utils\n",
    "\n",
    "# Word limits for normal and brief answers.\n",
    "normal_word_limit = 120\n",
    "brief_word_limit = 20\n",
    "# A word is \"between 5 and 6.5 characters per word including spaces and punctuation\":\n",
    "# https://charactercounter.com/characters-to-words\n",
    "normal_max_ch_soft = normal_word_limit * 6\n",
    "brief_max_ch_soft = brief_word_limit * 6\n",
    "# Hard limit adds 20% buffer and divides by 4 to get LLM token limit.\n",
    "# https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "normal_max_tokens_hard = int(normal_max_ch_soft * 1.2 / 4)\n",
    "brief_max_tokens_hard = int(brief_max_ch_soft * 1.2 / 4)\n",
    "\n",
    "normal_prompt = f'''Answer the user prompt below \"---\" line. Never exceed {normal_max_ch_soft} characters / {normal_word_limit} words.\n",
    "---\n",
    "'''\n",
    "\n",
    "brief_prompt = f'''Given the user prompt and a normal answer, generate a brief answer. A brief answer should be as short as possible but still answer the question and give relevant information. Never exceed {brief_max_ch_soft} characters / {brief_word_limit} words.\n",
    "Examples between --- lines:\n",
    "--- Example 1 ---\n",
    "Your input:\n",
    "Question: How much is 2+3?\n",
    "Normal answer: Expression 2+3 is equal to 5.\n",
    "Your output:\n",
    "5\n",
    "--- Example 2 ---\n",
    "Your input:\n",
    "Question: What is the color of the sky?\n",
    "Normal answer: The sky is blue.\n",
    "Your output:\n",
    "Blue\n",
    "--- End of examples\n",
    "\n",
    "Considering all the above, give a brief answer to the prompt and normal answer below:\n",
    "'''\n",
    "\n",
    "def brief_prompt(example):\n",
    "    q = example['question']\n",
    "    na = example['Answer-normal']\n",
    "    return f\"{brief_prompt}Question: {q}\\nNormal answer: {na}\"\n",
    "\n",
    "out_jsonl = \"1_dataset_creator/phi3_arena_brief_dataset.jsonl\"\n",
    "with open(out_jsonl, 'w') as f: pass\n",
    "\n",
    "normal_dataset = dataset.select_columns(['question_id', 'conversation_a']) \n",
    "normal_dataset = normal_dataset.map(\n",
    "        lambda example: {'question': example['conversation_a'][0]['content']}) \n",
    "normal_dataset = normal_dataset.map(\n",
    "        lambda example: {'prompt': normal_prompt + example['question']})\n",
    "\n",
    "timer = utils.Timer(f\"Processing first 100...\")\n",
    "with timer:\n",
    "    for normal_chunk in llm.process_variable_chunks(normal_dataset, tokenizer, model, 100, 5000, normal_max_tokens_hard):\n",
    "        normal_chunk = normal_chunk.select_columns(['question_id', 'question', 'answer'])\n",
    "        normal_chunk = normal_chunk.rename_column('answer', 'Answer-normal')\n",
    "        normal_chunk = normal_chunk.map(\n",
    "            lambda example: {'prompt':  brief_prompt(example)})\n",
    "        \n",
    "        for brief_chunk in llm.process_variable_chunks(normal_chunk, tokenizer, model, 100, 5000, brief_max_tokens_hard):\n",
    "            brief_chunk = brief_chunk.select_columns(['question_id', 'question', 'answer', 'Answer-normal'])\n",
    "            brief_chunk = brief_chunk.rename_column('answer', 'Answer-short')\n",
    "            with open(out_jsonl, 'ab') as f:\n",
    "                brief_chunk.to_json(f, lines=True)\n",
    "        \n",
    "        break\n",
    "\n",
    "print(f\"Time/sample: {timer.last_elapsed_time/100:.2f} sec\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
