{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9128f193c42145778edf39498f8f9e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Generator\n",
    "import utils.llm_utils as llm\n",
    "import datasets # type: ignore\n",
    "\n",
    "input_dataset = 'lmsys/chatbot_arena_conversations'\n",
    "dataset = datasets.load_dataset(input_dataset)['train']\n",
    "n_rows = len(dataset)\n",
    "print(str(n_rows))\n",
    "\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer, model = llm.load_tokenizer_and_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03c30c1e79245c0b24b94bb7f6ecce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.llm_utils' has no attribute 'process_variable_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mselect_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconversation_a\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m     10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m add_question_column(dataset) \n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m big_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_variable_chunks\u001b[49m(dataset, tokenizer, model, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1500\u001b[39m, \u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m     13\u001b[0m     big_chunk \u001b[38;5;241m=\u001b[39m big_chunk\u001b[38;5;241m.\u001b[39mselect_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_jsonl, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils.llm_utils' has no attribute 'process_variable_chunks'"
     ]
    }
   ],
   "source": [
    "def add_question_column(dataset: datasets.Dataset) -> datasets.Dataset:\n",
    "    return dataset.map(\n",
    "        lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "# Test:\n",
    "out_jsonl = \"1_dataset_creator/phi3_arena_brief_dataset.jsonl\"\n",
    "with open(out_jsonl, 'w') as f: pass\n",
    "\n",
    "dataset = dataset.select_columns(['question_id', 'conversation_a']) \n",
    "dataset = add_question_column(dataset) \n",
    "\n",
    "for big_chunk in llm.process_variable_chunks(dataset, tokenizer, model, 100, 1500, 50):\n",
    "    big_chunk = big_chunk.select_columns(['question_id', 'question', 'answer'])\n",
    "    with open(out_jsonl, 'ab') as f:\n",
    "        big_chunk.to_json(f, lines=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
