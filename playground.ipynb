{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88fca5415604736b26c05efbe0073e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Generator\n",
    "import utils.llm_utils as llm\n",
    "import datasets # type: ignore\n",
    "\n",
    "input_dataset = 'lmsys/chatbot_arena_conversations'\n",
    "dataset = datasets.load_dataset(input_dataset)['train']\n",
    "n_rows = len(dataset)\n",
    "print(str(n_rows))\n",
    "\n",
    "model_name = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "tokenizer, model = llm.load_tokenizer_and_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:\n",
    "out_jsonl = \"1_dataset_creator/phi3_arena_brief_dataset.jsonl\"\n",
    "with open(out_jsonl, 'w') as f: pass\n",
    "\n",
    "dataset = dataset.select_columns(['question_id', 'conversation_a']) \n",
    "dataset = dataset.map(\n",
    "        lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "for normal_chunk in llm.process_variable_chunks(dataset, tokenizer, model, 100, 1500, 50):\n",
    "    normal_chunk = normal_chunk.select_columns(['question_id', 'question', 'answer'])\n",
    "    with open(out_jsonl, 'ab') as f:\n",
    "        normal_chunk.to_json(f, lines=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 of 33000...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1709b3d0ebbf459e898cf2f2f77a4c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f9a019ef894924925f1d77f1247311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6b91193c964d3d955a96bd43c611e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e989000b834abd85c3f0ad014be8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccfbcb1c5e44807bc89a680c0ee9e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5441b092a0a04c85b6cee05a7de4fd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550efb3807504fc586fa0798c93d5c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2652e34fbba7451aa17b42a27407feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8660313fc34f378bce21d119307e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d19936abd3a43babdde686a4326324f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7608745b9d0e49dcbb2e5b94fade9739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda172966609448db18e84d09f9fb50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7453757927d4d7d8fc3eacb0f41511f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bf944440d74151a794e67304d8a174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3f04be2c334181a636508576c585ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time/sample: 0.53 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Word limits for normal and brief answers.\n",
    "normal_word_limit = 120\n",
    "brief_word_limit = 20\n",
    "# A word is \"between 5 and 6.5 characters per word including spaces and punctuation\":\n",
    "# https://charactercounter.com/characters-to-words\n",
    "normal_max_ch_soft = normal_word_limit * 6\n",
    "brief_max_ch_soft = brief_word_limit * 6\n",
    "# Hard limit adds 20% buffer and divides by 4 to get LLM token limit.\n",
    "# https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "normal_max_tokens_hard = int(normal_max_ch_soft * 1.2 / 4)\n",
    "brief_max_tokens_hard = int(brief_max_ch_soft * 1.2 / 4)\n",
    "\n",
    "normal_prompt = f'''Answer the user prompt below \"---\" line. Never exceed {normal_max_ch_soft} characters / {normal_word_limit} words.\n",
    "---\n",
    "'''\n",
    "\n",
    "brief_prompt = f'''Given the user prompt and a normal answer, generate a brief answer. A brief answer should be as short as possible but still answer the question and give relevant information. Never exceed {brief_max_ch_soft} characters / {brief_word_limit} words.\n",
    "Examples between --- lines:\n",
    "--- Example 1 ---\n",
    "Your input:\n",
    "Question: How much is 2+3?\n",
    "Normal answer: Expression 2+3 is equal to 5.\n",
    "Your output:\n",
    "5\n",
    "--- Example 2 ---\n",
    "Your input:\n",
    "Question: What is the color of the sky?\n",
    "Normal answer: The sky is blue.\n",
    "Your output:\n",
    "Blue\n",
    "--- End of examples\n",
    "\n",
    "Considering all the above, give a brief answer to the prompt and normal answer below:\n",
    "'''\n",
    "\n",
    "def brief_prompt(example):\n",
    "    q = example['question']\n",
    "    na = example['Answer-normal']\n",
    "    return f\"{brief_prompt}Question: {q}\\nNormal answer: {na}\"\n",
    "\n",
    "out_jsonl = \"1_dataset_creator/phi3_arena_brief_dataset.jsonl\"\n",
    "with open(out_jsonl, 'w') as f: pass\n",
    "\n",
    "normal_dataset = dataset.select_columns(['question_id', 'conversation_a']) \n",
    "normal_dataset = normal_dataset.map(\n",
    "        lambda example: {'question': example['conversation_a'][0]['content']}) \n",
    "normal_dataset = normal_dataset.map(\n",
    "        lambda example: {'prompt': normal_prompt + example['question']})\n",
    "\n",
    "start_time = time.time()\n",
    "current_example = 0\n",
    "print(f\"Example {current_example} of {n_rows}...\")\n",
    "for normal_chunk in llm.process_variable_chunks(normal_dataset, tokenizer, model, 100, 5000, normal_max_tokens_hard):\n",
    "    normal_chunk = normal_chunk.select_columns(['question_id', 'question', 'answer'])\n",
    "    normal_chunk = normal_chunk.rename_column('answer', 'Answer-normal')\n",
    "    normal_chunk = normal_chunk.map(\n",
    "        lambda example: {'prompt':  brief_prompt(example)})\n",
    "    \n",
    "    for brief_chunk in llm.process_variable_chunks(normal_chunk, tokenizer, model, 100, 5000, brief_max_tokens_hard):\n",
    "        brief_chunk = brief_chunk.select_columns(['question_id', 'question', 'answer', 'Answer-normal'])\n",
    "        brief_chunk = brief_chunk.rename_column('answer', 'Answer-short')\n",
    "        with open(out_jsonl, 'ab') as f:\n",
    "            brief_chunk.to_json(f, lines=True)\n",
    "\n",
    "    last_elapsed_time = time.time() - start_time\n",
    "    print(f\"Time/sample: {last_elapsed_time/len(normal_chunk):.2f} sec\") \n",
    "    current_example += len(normal_chunk)\n",
    "    \n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
