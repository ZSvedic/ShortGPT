{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "in_jsonl = '../2_brevity_benchmark/phi3_arena_winners.jsonl'\n",
    "dataset = datasets.load_dataset('json', data_files=in_jsonl, split='train')\n",
    "n_rows = len(dataset)\n",
    "print(str(n_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26968"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter out duplicates.\n",
    "def filter_out_duplicates(example, seen_questions = set()):\n",
    "    if example['question'] in seen_questions:\n",
    "        return False\n",
    "    else:\n",
    "        seen_questions.add(example['question'])\n",
    "        return True\n",
    "\n",
    "dataset = dataset.filter(filter_out_duplicates)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to filter out errors, normal answers, and \n",
    "# short answers that are not much shorter than normal answers.\n",
    "def filter_out_bad_data(example):\n",
    "    name_best, ans_best, ans_normal = example['name-best'], example['answer-best'], example['answer-normal']\n",
    "    # Include only if:\n",
    "    return name_best!='ERROR' and name_best=='short' and len(ans_best)<0.6*len(ans_normal)\n",
    "\n",
    "dataset = dataset.filter(filter_out_bad_data)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0394143c094119aedb0a56a258861b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question-id': '58210e39b3fd4441a2bd4a518bb44c2d',\n",
       " 'prompt': 'What is the difference between OpenCL and CUDA?',\n",
       " 'chosen': 'CUDA architecture and optimizations.',\n",
       " 'rejected': 'OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture) are both parallel computing platforms and programming models designed for general-purpose computing on GPUs (GPGPU). The main differences between them are:\\n\\n1. Platform and vendor: OpenCL is an open standard developed by the Khronos Group, which allows developers to write programs that can run on various hardware platforms, including GPUs from different vendors like NVIDIA, AMD, and Intel. CUDA, on the other hand, is a proprietary platform developed by NVIDIA, primarily targeting NVIDIA GPUs.\\n\\n2. Language: OpenCL uses a C-like language for writing kernels, which are functions executed on the GPU. CUDA also uses a C-like language, but it has additional features and extensions specific to NVIDIA GPUs.\\n\\n3. Performance: CUDA is generally faster than OpenCL on NVIDIA GPUs due to its close integration with the'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename columns to match DPO format:\n",
    "# https://huggingface.co/docs/trl/dpo_trainer#expected-dataset-format\n",
    "dataset = dataset.map(lambda row: {\n",
    "    'question-id': row['question_id'],\n",
    "    'prompt': row['question'],\n",
    "    'chosen': row['answer-short'],\n",
    "    'rejected': row['answer-normal'],\n",
    "    })\n",
    "dataset = dataset.remove_columns([\n",
    "    'question_id', 'question', 'name-best', 'answer-best', 'answer-normal', 'answer-short'])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eee25bc0d3a4e5eb96b26a9e620b2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18495200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_jsonl = 'phi3-arena-short-dpo-train.jsonl'\n",
    "dataset.to_json(out_jsonl, lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
