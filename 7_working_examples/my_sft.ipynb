{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDo:\n",
    "- Validation loss is 'nan', ChatGPT: \"Yes. For validation perplexity or cross‑entropy, you need reference answers. Keep the full user+assistant in the eval split and mask user tokens (label = ‑100). Then you won’t get NaN during validation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token: <|endoftext|>\n",
      "eos_token: <|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import time, trl, torch, datasets, wandb, transformers as tr\n",
    "from pprint import pprint\n",
    "from typing import Optional\n",
    "\n",
    "# LoRA:\n",
    "# python sft.py \\\n",
    "#     --model_name_or_path Qwen/Qwen2-0.5B \\\n",
    "#     --dataset_name trl-lib/Capybara \\\n",
    "#     --learning_rate 2.0e-4 \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --packing \\\n",
    "#     --per_device_train_batch_size 2 \\\n",
    "#     --gradient_accumulation_steps 8 \\\n",
    "#     --gradient_checkpointing \\\n",
    "#     --logging_steps 8 \\\n",
    "#     --eval_strategy steps \\\n",
    "#     --eval_steps 16 \\\n",
    "#     --use_peft \\\n",
    "#     --lora_r 32 \\\n",
    "#     --lora_alpha 16 \\\n",
    "#     --output_dir Qwen2-0.5B-SFT \\\n",
    "\n",
    "# Arguments:\n",
    "\n",
    "script_args = trl.ScriptArguments(\n",
    "    # dataset_name='trl-lib/Capybara')\n",
    "    dataset_name='ZSvedic/gpt4o-arena-brevity-dpo')\n",
    "\n",
    "model_args = trl.ModelConfig(\n",
    "    # model_name_or_path='Qwen/Qwen2-0.5B',\n",
    "    model_name_or_path='Qwen/Qwen2-0.5B-Instruct',\n",
    "    use_peft=True,\n",
    "    lora_r=32,\n",
    "    lora_alpha=16)\n",
    "\n",
    "training_args = trl.SFTConfig(\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1,\n",
    "    packing=False, # Changed from True, to make sense of debugging.\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=5,\n",
    "    report_to=\"wandb\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    output_dir=\"OUTPUT/Qwen2-0.5B-SFT\",\n",
    "    run_name=f\"Qwen2-0.5B-SFT-{time.strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    )\n",
    "\n",
    "training_args.model_init_kwargs = dict(\n",
    "    use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    device_map=trl.get_kbit_device_map(),\n",
    "    quantization_config=trl.get_quantization_config(model_args),\n",
    ")\n",
    "\n",
    "# Tokenizer:\n",
    "tokenizer = tr.AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "# tokenizer.pad_token = tokenizer.eos_token # Is this needed?\n",
    "print(f\"pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"eos_token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question-id', 'messages'],\n",
      "        num_rows: 22941\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question-id', 'messages'],\n",
      "        num_rows: 2549\n",
      "    })\n",
      "})\n",
      "TRAIN EXAMPLE: {'question-id': '1dd6137eb3c3470989e18ab729ccc0b3', 'messages': [{'content': 'write short telugu poem', 'role': 'user'}, {'content': 'Telugu poem: ఆకాశం నీలమై పూగుతోంది, సూర్యుడు కొత్త కిరణాలు తెచ్చుకొన్నాడు.', 'role': 'assistant'}]}\n",
      "TEST EXAMPLE: {'question-id': '7e730384bbb649af9f6e150dbf129b53', 'messages': [{'content': 'What is your purpose.', 'role': 'user'}]}\n",
      "chat_template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "mapped_dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 22941\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2549\n",
      "    })\n",
      "})\n",
      "TRAIN EXAMPLE 'text': <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "write short telugu poem<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Telugu poem: ఆకాశం నీలమై పూగుతోంది, సూర్యుడు కొత్త కిరణాలు తెచ్చుకొన్నాడు.<|im_end|>\n",
      "\n",
      "TEST EXAMPLE 'text': <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your purpose.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading.\n",
    "dataset = datasets.load_dataset(script_args.dataset_name)\n",
    "\n",
    "# Train messages.\n",
    "dataset['train'] = dataset['train'].map(\n",
    "    lambda row: {'messages': [\n",
    "        {'role': 'user', 'content': row['prompt']},\n",
    "        {'role': 'assistant', 'content': row['chosen']}]},\n",
    "    remove_columns = ['prompt', 'chosen', 'rejected'])\n",
    "\n",
    "# Test messages.\n",
    "dataset['test'] = dataset['test'].map(\n",
    "    lambda row: {'messages': [\n",
    "        {'role': 'user', 'content': row['prompt']}]},\n",
    "    remove_columns = ['prompt', 'chosen', 'rejected'])\n",
    "\n",
    "# Debug print.\n",
    "print(f\"dataset: {dataset}\")\n",
    "print(f\"TRAIN EXAMPLE: {dataset['train'][0]}\")\n",
    "print(f\"TEST EXAMPLE: {dataset['test'][0]}\")\n",
    "print(f\"chat_template: {tokenizer.chat_template}\")\n",
    "\n",
    "# Tokenize the dataset.\n",
    "mapped_dataset = datasets.DatasetDict()\n",
    "\n",
    "mapped_dataset['train'] = dataset['train'].map(\n",
    "    lambda x: {\n",
    "        \"text\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False, \n",
    "                                              add_generation_prompt=False)\n",
    "        },\n",
    "    remove_columns=dataset['train'].column_names)\n",
    "\n",
    "mapped_dataset['test'] = dataset['test'].map(\n",
    "    lambda x: {\n",
    "        \"text\": tokenizer.apply_chat_template(x[\"messages\"], tokenize=False, \n",
    "                                              add_generation_prompt=True)\n",
    "        },\n",
    "    remove_columns=dataset['test'].column_names)\n",
    "\n",
    "# Debug print.\n",
    "print(f\"mapped_dataset: {mapped_dataset}\")\n",
    "print(f\"TRAIN EXAMPLE 'text': {mapped_dataset['train'][0]['text']}\")\n",
    "print(f\"TEST EXAMPLE 'text': {mapped_dataset['test'][0]['text']}\")\n",
    "\n",
    "# Data collator makes sure we train on completions only.\n",
    "collator = trl.DataCollatorForCompletionOnlyLM(\n",
    "    instruction_template=\"<|im_start|>user\\n\", \n",
    "    response_template=\"<|im_start|>assistant\\n\", \n",
    "    tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzsvedic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zel/ml-projects/DPO-examples/sft-official/wandb/run-20250204_113229-ufh6zu4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zsvedic/huggingface/runs/ufh6zu4g' target=\"_blank\">Qwen2-0.5B-SFT-2025-02-04-11-32</a></strong> to <a href='https://wandb.ai/zsvedic/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zsvedic/huggingface' target=\"_blank\">https://wandb.ai/zsvedic/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zsvedic/huggingface/runs/ufh6zu4g' target=\"_blank\">https://wandb.ai/zsvedic/huggingface/runs/ufh6zu4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='1433' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  21/1433 02:24 < 2:58:41, 0.13 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.170700</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.667700</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.733800</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='319' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/319 00:07 < 00:11, 16.68 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "You are a prodessional physic teacher who knows a lot about physic, describe each step in detail and gives the reequired formula or equation in forms of latex\n",
      "It is proposed to transmit a signal over a distance of 4.5 × 103 km by means of an optic fibre. The input signal has a power of 9.8 mW. The minimum signal that can be detected at the output has a power of 6.3 × 10–17 W. For this signal power, the signal‐to‐noise ratio is 21 dB. Calculate(Use Wolfram alpha if needed): the power of the background noise.<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=============================\n",
      "\n",
      "\n",
      "I want to do some interactive instruction. I want you to start explaining the concept of rocket busters engine capacity limits to me at a 10th grade level. Then stop, give me a multiple choice quiz, grade the quiz, and resume the explanation. If I get the quiz wrong, reduce the grade level by 3 for the explanation and language you use, making the language simpler. Otherwise increase it by three and make the language harder. Then quiz me again and repeat the process.<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "\n",
      "### Human: Do an entity extraction and give the output in json:\n",
      "The president of the United States is the head of state and head of government of the United States,[1] indirectly elected to a four-year term via the Electoral College.[2] The officeholder leads the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.[3] Since the office was established in 1789, 45 men have served in 46 presidencies. The first president, George Washington, won a unanimous vote of the Electoral College;[4] one, Grover Cleveland, served two non-consecutive terms and is therefore counted as the 22nd and 24th president of the United States, giving rise to the discrepancy between the number of presidencies and the number of persons who have served as president.[5] The incumbent president is Joe Biden.[6]\n",
      "\n",
      "The presidency of William Henry Harrison, who died 31 days after taking office in 1841, was the shortest in American history.[7] Franklin D. Roosevelt served the longest, over twelve years, before dying early in his fourth term in 1945. He is the only U.S. president to have served more than two terms.[8] Since the ratification of the Twenty-second Amendment to the United States Constitution in 1951, no person may be elected president more than twice, and no one who has served more than two years of a term to which someone else was elected may be elected more than once.\n",
      "###  Assistant:<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "please point out  the typora :\n",
      "\n",
      "      \\caption{ The MRR@10 performance of the models across retrieval tasks. Some tasks are extremely hard or easy for all models.  }\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "A farmer went to a market and purchased a wolf, a goat, and a cabbage. On his way home, the farmer came to the bank of a river and rented a boat. But crossing the river by boat, the farmer could carry only himself and a single one of his purchases: the wolf, the goat, or the cabbage. If left unattended together, the wolf would eat the goat, or the goat would eat the cabbage. Go through solving this problem step by step, describe how the farmer can get himself and all of his items to the far side of the river by boat.<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "******* ALL OF MY LOADS REQUIRE MACRO POINT TRACKING=******\n",
      " \n",
      "Pu:  st charles il \n",
      "Del:  bucyrus oh \n",
      " \n",
      "Rdy:  1000et today\n",
      "Del:  by 2200 tonight or 0800 et 1-5 let me kno= \n",
      "12 at 17400lbs no stack\n",
      "48x42x20\n",
      "Let me know\n",
      "John\n",
      "\n",
      "Extract pickup_date, pickup_time, drop_date, drop_time, quantity, weight, stackable_flag, dimensions from the above email body<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:196: UserWarning: Could not find response key `<|im_start|>assistant\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What seems to be the problem of the participant number 215313906 and how are the others trying to help in the following discussion: OP\n",
      "215313906\n",
      "17 h\n",
      "\n",
      "\n",
      "81\n",
      "\n",
      "21\n",
      "\n",
      " \n",
      "Aasialainen kaveri tulossa kesällä Heseen käymään ja aivan innosta soikeana selitti kuinka innoissaan hän on kun pääsee kokemaan autenttista suomalaista kulttuuria ja keittiötä. En vastannut mitään.\n",
      "263 replies by 201 users\n",
      "1\n",
      "215313953\n",
      "17 h\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "00:07\n",
      "3\n",
      "2\n",
      "215313989\n",
      "17 h\n",
      "\n",
      "\n",
      "83\n",
      "\n",
      "5\n",
      "\n",
      "Juota se kännii ja opeta sitomaan hirtto solmukka, muuta ei oikee tarvi\n",
      "1\n",
      "3\n",
      "215314015\n",
      "17 h\n",
      "\n",
      "\n",
      "63\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "Laittahan rössypottu tulille.\n",
      "2\n",
      "4\n",
      "215314078\n",
      "17 h\n",
      "\n",
      "\n",
      "46\n",
      "\n",
      "\n",
      "≫215313989\n",
      "Pitää raahata myös saunaan ja mielellään oikein kylmänä kesäpäivänä että sen voi samalla potkasta laiturilta uimaan johonkin ~10 asteiseen järveen\n",
      "1\n",
      "OP\n",
      "215314096\n",
      "17 h\n",
      "\n",
      "\n",
      "21\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "Aasian maissa on tyypillistä että ravintoloissa syöminen on halpaa ja niistä saa paikallista ruokaa useita eri annoksia, katuruoan monipuolisuudesta puhumattakaan.\n",
      "9\n",
      "5\n",
      "215314126\n",
      "17 h\n",
      "\n",
      "\n",
      "133\n",
      "\n",
      "\n",
      " \n",
      "tällaset teet niin se jää tänne\n",
      "11\n",
      "OP\n",
      "215314137\n",
      "17 h\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      " \n",
      "≫215314015\n",
      "Ei tommosta saa hesestä.\n",
      "3\n",
      "3\n",
      "215314164\n",
      "17 h\n",
      "\n",
      "\n",
      "67\n",
      "\n",
      "6\n",
      "\n",
      " \n",
      "vuan on hyvvöö\n",
      "10\n",
      "6\n",
      "215314169\n",
      "17 h\n",
      "\n",
      "\n",
      "47\n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "Vii hääv tis ting kolled salmiakki and sisu and kala kukko haha jees\n",
      "2\n",
      "OP\n",
      "215314179\n",
      "17 h\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "≫215314126\n",
      "1\n",
      "4\n",
      "215314194\n",
      "17 h\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "\n",
      "≫215314078\n",
      "Eikä saa unohtaa hk:n sinistä kiukaalla ja desiä sinappia päälle\n",
      "1\n",
      "7\n",
      "215314209\n",
      "17 h\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "hiiva suuhun\n",
      "5\n",
      "215314226\n",
      "17 h\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      " \n",
      "≫215314179\n",
      "teeppä\n",
      "8\n",
      "215314234\n",
      "17 h\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Syötät sille mämmiä ja mustaamakkaraa sekä juotat viinaa\n",
      "3\n",
      "3\n",
      "215314250\n",
      "17 h\n",
      "\n",
      "\n",
      "18\n",
      "\n",
      "\n",
      "≫215314137\n",
      "On kyllä paska paikka sit.\n",
      "1\n",
      "9\n",
      "215314253\n",
      "17 h\n",
      "\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "Vie se metrolla Kontulaan ja hae viikon päästä takaisin\n",
      "1\n",
      "10\n",
      "215314254\n",
      "17 h\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Tarjoat homocityläistä perinneruokaa, neekerin spermaa asfaltilta\n",
      "2\n",
      "11\n",
      "215314259\n",
      "17 h\n",
      "\n",
      "\n",
      "24\n",
      "\n",
      "\n",
      "≫215314126\n",
      "Huutis joka kerta soosin määrälle xDD\n",
      "3\n",
      "12\n",
      "215314262\n",
      "17 h\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      " \n",
      "Katsotte Pohjanmaan kännissä ja olette hiljaa. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What seems to be the problem of the participant number 215313906 and how are the others trying to help in the following discussion: OP\n",
      "215313906\n",
      "17 h\n",
      "\n",
      "\n",
      "81\n",
      "\n",
      "21\n",
      "\n",
      " \n",
      "Aasialainen kaveri tulossa kesällä Heseen käymään ja aivan innosta soikeana selitti kuinka innoissaan hän on kun pääsee kokemaan autenttista suomalaista kulttuuria ja keittiötä. En vastannut mitään.\n",
      "263 replies by 201 users\n",
      "1\n",
      "215313953\n",
      "17 h\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "00:07\n",
      "3\n",
      "2\n",
      "215313989\n",
      "17 h\n",
      "\n",
      "\n",
      "83\n",
      "\n",
      "5\n",
      "\n",
      "Juota se kännii ja opeta sitomaan hirtto solmukka, muuta ei oikee tarvi\n",
      "1\n",
      "3\n",
      "215314015\n",
      "17 h\n",
      "\n",
      "\n",
      "63\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "Laittahan rössypottu tulille.\n",
      "2\n",
      "4\n",
      "215314078\n",
      "17 h\n",
      "\n",
      "\n",
      "46\n",
      "\n",
      "\n",
      "≫215313989\n",
      "Pitää raahata myös saunaan ja mielellään oikein kylmänä kesäpäivänä että sen voi samalla potkasta laiturilta uimaan johonkin ~10 asteiseen järveen\n",
      "1\n",
      "OP\n",
      "215314096\n",
      "17 h\n",
      "\n",
      "\n",
      "21\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "Aasian maissa on tyypillistä että ravintoloissa syöminen on halpaa ja niistä saa paikallista ruokaa useita eri annoksia, katuruoan monipuolisuudesta puhumattakaan.\n",
      "9\n",
      "5\n",
      "215314126\n",
      "17 h\n",
      "\n",
      "\n",
      "133\n",
      "\n",
      "\n",
      " \n",
      "tällaset teet niin se jää tänne\n",
      "11\n",
      "OP\n",
      "215314137\n",
      "17 h\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "1\n",
      "\n",
      " \n",
      "≫215314015\n",
      "Ei tommosta saa hesestä.\n",
      "3\n",
      "3\n",
      "215314164\n",
      "17 h\n",
      "\n",
      "\n",
      "67\n",
      "\n",
      "6\n",
      "\n",
      " \n",
      "vuan on hyvvöö\n",
      "10\n",
      "6\n",
      "215314169\n",
      "17 h\n",
      "\n",
      "\n",
      "47\n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "Vii hääv tis ting kolled salmiakki and sisu and kala kukko haha jees\n",
      "2\n",
      "OP\n",
      "215314179\n",
      "17 h\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "≫215314126\n",
      "1\n",
      "4\n",
      "215314194\n",
      "17 h\n",
      "\n",
      "\n",
      "25\n",
      "\n",
      "\n",
      "≫215314078\n",
      "Eikä saa unohtaa hk:n sinistä kiukaalla ja desiä sinappia päälle\n",
      "1\n",
      "7\n",
      "215314209\n",
      "17 h\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "hiiva suuhun\n",
      "5\n",
      "215314226\n",
      "17 h\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      " \n",
      "≫215314179\n",
      "teeppä\n",
      "8\n",
      "215314234\n",
      "17 h\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Syötät sille mämmiä ja mustaamakkaraa sekä juotat viinaa\n",
      "3\n",
      "3\n",
      "215314250\n",
      "17 h\n",
      "\n",
      "\n",
      "18\n",
      "\n",
      "\n",
      "≫215314137\n",
      "On kyllä paska paikka sit.\n",
      "1\n",
      "9\n",
      "215314253\n",
      "17 h\n",
      "\n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "Vie se metrolla Kontulaan ja hae viikon päästä takaisin\n",
      "1\n",
      "10\n",
      "215314254\n",
      "17 h\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "6\n",
      "\n",
      "Tarjoat homocityläistä perinneruokaa, neekerin spermaa asfaltilta\n",
      "2\n",
      "11\n",
      "215314259\n",
      "17 h\n",
      "\n",
      "\n",
      "24\n",
      "\n",
      "\n",
      "≫215314126\n",
      "Huutis joka kerta soosin määrälle xDD\n",
      "3\n",
      "12\n",
      "215314262\n",
      "17 h\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      " \n",
      "Katsotte Pohjanmaan kännissä ja olette hiljaa. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "\n",
      "please point out  the typora :\n",
      "\n",
      "      \\caption{ The MRR@10 performance of the models across retrieval tasks. Some tasks are extremely hard or easy for all models.  }\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\n",
      "\n",
      "Return a CSV entry with fields for whether it necessitates a command, and what your response or the command's fields are. For example:\n",
      "\"\"\"\n",
      "John doe: Mute bob for 3 seconds\n",
      "\"\"\"\n",
      "1,mute,\"bob\",3000\n",
      "\n",
      "\"\"\"\n",
      "Alice: I don't like math\n",
      "Bob: e5x, what do you think of math\n",
      "\"\"\"\n",
      "0,\"As a chatbot, I personally hate math.\"\n",
      "\n",
      "\"\"\"\n",
      "Alice: What is array.slice\n",
      "\"\"\"\n",
      "1,mdn,\"array.slice\"\n",
      "\n",
      "\"\"\"\n",
      "Daniel: Run the \"time\" command.\n",
      "\"\"\"\n",
      "0,\"I don't have a command like that, but I think it's around ${new Date().toDateString()}\"\n",
      "\n",
      "This conversation is your prompt, respond to the last message. Be snarky.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "0,\"Try a real command next time. Dream big!\"<|im_end|>\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/home/zel/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/trl/trainer/utils.py:211: UserWarning: Could not find instruction key `<|im_start|>user\n",
      "` in the following instance: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "\t\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "0000\n",
      "0100\n",
      "Icon\n",
      "Partly cloudy and cirrus\n",
      "Mostly cloudy and few cirrus\n",
      "Overcast\n",
      "Overcast\n",
      "Overcast\n",
      "Overcast\n",
      "Overcast\n",
      "(°C)\n",
      "20°\n",
      "19°\n",
      "17°\n",
      "16°\n",
      "15°\n",
      "14°\n",
      "14°\n",
      "(°C)\n",
      "15°\n",
      "14°\n",
      "13°\n",
      "11°\n",
      "10°\n",
      "10°\n",
      "9°\n",
      "ESE\n",
      "ESE\n",
      "ESE\n",
      "ESE\n",
      "ESE\n",
      "ESE\n",
      "ESE\n",
      "(km/h)\n",
      "20-30\n",
      "18-27\n",
      "16-26\n",
      "17-22\n",
      "17-28\n",
      "16-30\n",
      "17-32\n",
      "(mm)\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "(%)\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n",
      "---\n",
      "\n",
      "Tell me what the temperature (C) will be at 00:00 based on data above?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "14°C<|im_end|>\n",
      ". This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m trainer\u001b[38;5;241m.\u001b[39madd_callback(len_callback)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:2598\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2598\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:3071\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3069\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3071\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3072\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:3025\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3025\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3028\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:4073\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4070\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4072\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4073\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4074\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   4077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   4078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4083\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/trainer.py:4257\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4254\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4256\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[0;32m-> 4257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m   4258\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[1;32m   4259\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[1;32m   4260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/accelerate/data_loader.py:572\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 572\u001b[0m         current_batch \u001b[38;5;241m=\u001b[39m \u001b[43msend_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_non_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m    574\u001b[0m     next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/accelerate/utils/operations.py:155\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    153\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu:0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:820\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    821\u001b[0m         k: v\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39mnon_blocking) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ml-projects/DPO-examples/sft-official/.venv-20250121/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:821\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_device(device) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 821\u001b[0m         k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    823\u001b[0m     }\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(device)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "class LogCompletionsLengthCallback(tr.TrainerCallback):\n",
    "    def __init__(self, trainer: tr.Trainer, num_prompts: Optional[int] = None, freq: Optional[int] = None):\n",
    "        self.trainer = trainer\n",
    "        self.freq = freq\n",
    "        self._last_logged_step = -1\n",
    "        self.eval_dataset = trainer.eval_dataset.select(range(num_prompts))\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Only log once per step (this method may be called multiple times)\n",
    "        if state.global_step == self._last_logged_step:\n",
    "            return\n",
    "\n",
    "        # Only log every `freq` steps (if no `freq` is provided, log every `eval_steps` steps)\n",
    "        freq = self.freq or state.eval_steps\n",
    "        if state.global_step % freq != 0:\n",
    "            return\n",
    "\n",
    "        tokenizer = kwargs[\"processing_class\"]\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        accelerator = self.trainer.accelerator\n",
    "        model = self.trainer.model_wrapped\n",
    "        completion_lens = []\n",
    "        with accelerator.split_between_processes(self.eval_dataset[\"input_ids\"]) as prompts:\n",
    "            with trl.models.utils.unwrap_model_for_generation(model, accelerator) as unwrapped_model:\n",
    "                for prompt_ids in prompts:\n",
    "                    prompt_ids = torch.tensor([prompt_ids], device=unwrapped_model.device)\n",
    "                    generations = unwrapped_model.generate(\n",
    "                        prompt_ids, generation_config=tr.GenerationConfig(max_new_tokens=150)\n",
    "                    )\n",
    "                    completion_lens.append(len(generations[0]) - len(prompt_ids[0]))\n",
    "\n",
    "        # Build the data to log\n",
    "        if self.trainer.accelerator.is_main_process:\n",
    "            wandb.log({\"completions_len\": sum(completion_lens) / len(completion_lens)}, step=state.global_step)\n",
    "\n",
    "        # Save the last logged step, so we don't log the same completions multiple times\n",
    "        self._last_logged_step = state.global_step\n",
    "\n",
    "# class InspectorSFTTrainer(SFTTrainer):\n",
    "#     def training_step(self, model, inputs, num_items_in_batch):\n",
    "#         input_ids = inputs['input_ids'][0]\n",
    "#         att_mask = inputs['attention_mask'][0]\n",
    "#         labels = inputs['labels'][0]\n",
    "#         num_ones = att_mask.sum().item()\n",
    "#         if num_ones < 100:\n",
    "#             print(f\"\\n------------------------\")\n",
    "#             print(f\"num_ones: {num_ones}\")\n",
    "#             decoded_tokens = self.processing_class.decode(input_ids, skip_special_tokens=False)\n",
    "#             print(f\"ALL TOKENS:\\n{decoded_tokens}\")\n",
    "#             valid_token_ids = input_ids[att_mask == 1]\n",
    "#             decoded_masked_tokens = self.processing_class.decode(valid_token_ids, skip_special_tokens=False)\n",
    "#             print(f\"MASK=1 TOKENS:\\n{decoded_masked_tokens}\")\n",
    "#             print(f\"LABELS: {labels}\")\n",
    "#             filtered_labels = labels[labels != -100]\n",
    "#             print(f\"LABELS decoded:\\n{self.processing_class.decode(filtered_labels, skip_special_tokens=True)}\")\n",
    "#             print(f\"\\n------------------------\")\n",
    "\n",
    "#         return super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "# # Initialize the custom trainer\n",
    "# trainer = InspectorSFTTrainer(\n",
    "trainer = SFTTrainer(\n",
    "    model=model_args.model_name_or_path,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=mapped_dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None,\n",
    "    data_collator=collator,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=trl.get_peft_config(model_args),\n",
    ")\n",
    "\n",
    "# Add any callbacks as needed\n",
    "# completion_callback = trl.LogCompletionsCallback(trainer, num_prompts=16)\n",
    "# trainer.add_callback(completion_callback)\n",
    "len_callback = LogCompletionsLengthCallback(trainer, num_prompts=16)\n",
    "trainer.add_callback(len_callback)\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LogCompletionsLengthCallback(tr.TrainerCallback):\n",
    "#     def __init__(self, trainer: tr.Trainer, num_prompts: Optional[int] = None, freq: Optional[int] = None):\n",
    "#         self.trainer = trainer\n",
    "#         self.freq = freq\n",
    "#         self._last_logged_step = -1\n",
    "#         self.eval_dataset = trainer.eval_dataset.select(range(num_prompts))\n",
    "\n",
    "#     def on_step_end(self, args, state, control, **kwargs):\n",
    "#         # Only log once per step (this method may be called multiple times)\n",
    "#         if state.global_step == self._last_logged_step:\n",
    "#             return\n",
    "\n",
    "#         # Only log every `freq` steps (if no `freq` is provided, log every `eval_steps` steps)\n",
    "#         freq = self.freq or state.eval_steps\n",
    "#         if state.global_step % freq != 0:\n",
    "#             return\n",
    "\n",
    "#         tokenizer = kwargs[\"processing_class\"]\n",
    "#         tokenizer.padding_side = \"left\"\n",
    "#         accelerator = self.trainer.accelerator\n",
    "#         model = self.trainer.model_wrapped\n",
    "#         completion_lens = []\n",
    "#         with accelerator.split_between_processes(self.eval_dataset[\"input_ids\"]) as prompts:\n",
    "#             with trl.models.utils.unwrap_model_for_generation(model, accelerator) as unwrapped_model:\n",
    "#                 for prompt_ids in prompts:\n",
    "#                     prompt_ids = torch.tensor([prompt_ids], device=unwrapped_model.device)\n",
    "#                     generations = unwrapped_model.generate(\n",
    "#                         prompt_ids, generation_config=tr.GenerationConfig(max_new_tokens=150)\n",
    "#                     )\n",
    "#                     completion_lens.append(len(generations[0]) - len(prompt_ids[0]))\n",
    "\n",
    "#         # Build the data to log\n",
    "#         if self.trainer.accelerator.is_main_process:\n",
    "#             wandb.log({\"completions_len\": sum(completion_lens) / len(completion_lens)}, step=state.global_step)\n",
    "\n",
    "#         # Save the last logged step, so we don't log the same completions multiple times\n",
    "#         self._last_logged_step = state.global_step\n",
    "        \n",
    "# # Training:\n",
    "# trainer = trl.SFTTrainer(\n",
    "#     model=model_args.model_name_or_path,\n",
    "#     args=training_args,\n",
    "#     train_dataset=dataset[script_args.dataset_train_split],\n",
    "#     eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != \"no\" else None,\n",
    "#     processing_class=tokenizer,\n",
    "#     peft_config=trl.get_peft_config(model_args),\n",
    "# )\n",
    "# # len_callback = LogCompletionsLengthCallback(trainer, num_prompts=16)\n",
    "# # trainer.add_callback(len_callback)\n",
    "# completion_callback = trl.LogCompletionsCallback(trainer, num_prompts=16)\n",
    "# trainer.add_callback(completion_callback)\n",
    "\n",
    "# print(f\"pad_token: {tokenizer.pad_token}, eos_token: {tokenizer.eos_token}\")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Save and push to hub\n",
    "# trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-20250121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
