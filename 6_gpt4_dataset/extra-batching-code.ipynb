{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(start, increment, total): \n",
    "    ''' Yields batch sizes for `total` items, with `start` index and `increment` batch size. \\n\n",
    "    Example 1: batcher(1, 20, 50) -> [20, 20, 9] \\n\n",
    "    Example 2: \n",
    "        for i, size in enumerate(batcher(1, 20, 50)):\n",
    "            print(f\"Batch {i+1} of size {size}: [{i*20+1}:{i*20+size}]\")\n",
    "    '''\n",
    "    if start+increment > total:\n",
    "        yield total-start\n",
    "    else:\n",
    "        yield increment\n",
    "        yield from batcher(start+increment, increment, total)\n",
    "\n",
    "print(list(batcher(1, 20, 50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, size in enumerate(batcher(ds_skip_rows, batch_size, ds_rows_to_process)):\n",
    "    start = ds_skip_rows + i*batch_size\n",
    "    end = start + size\n",
    "    print(f\"Processing batch {i+1} of size {size}: [{start}:{end}]\")\n",
    "    batch = oaib.Auto(workers=8)\n",
    "    for j, row in enumerate(ds.select(range(start, end))):\n",
    "        messages=[{\"role\": \"user\", \"content\": row['prompt']}]\n",
    "        await batch.add(\"chat.completions.create\", \n",
    "                        model=model_name, \n",
    "                        messages=messages, \n",
    "                        max_tokens=normal_max_tokens_hard)                       \n",
    "    output = await batch.run()\n",
    "    ds = ds.map(with_indices=True, \n",
    "                function=lambda row, idx: {\n",
    "                    'normal-answer': output.iloc[idx-start].result['choices'][0]['message']['content'] \n",
    "                        if start <= idx < end else None} )\n",
    "    pprint(ds[4]['normal-answer'])\n",
    "\n",
    "pprint(ds[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_batch_sizes(total_rows, batch_size, skip_rows):\n",
    "#     n_batches = math.ceil(total_rows / batch_size)\n",
    "#     batch_sizes = []\n",
    "#     start = skip_rows\n",
    "#     for _ in range(n_batches):\n",
    "#         end = min(start+batch_size, skip_rows+total_rows)\n",
    "#         batch_sizes.append((end-start, start, end))\n",
    "#         start = end\n",
    "#     return batch_sizes\n",
    "\n",
    "# # Example usage:\n",
    "# batch_sizes = calc_batch_sizes(ds_rows_to_process, batch_size, ds_skip_rows)\n",
    "# print(batch_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for b_i, (size, start, end) in enumerate(batch_sizes):\n",
    "#     print(f\"Processing batch {b_i+1} of {len(batch_sizes)}...\")\n",
    "#     batch_ds = ds.select(range(start, end))\n",
    "#     batch = Auto(workers=8)\n",
    "#     for j, row in enumerate(batch_ds):\n",
    "#         # print(f\"Batch {b_i}, row {j}, prompt: {row['prompt']}\")\n",
    "#         messages=[{\"role\": \"user\", \"content\": row['prompt']}]\n",
    "#         await batch.add(\"chat.completions.create\", model=model_name, \n",
    "#                         messages=messages, max_tokens=normal_max_tokens_hard)                       \n",
    "#     output = await batch.run()\n",
    "#     for row in output.itertuples():\n",
    "#         print(f\"{row.Index}: {batch_ds[row.Index]['question']}\")\n",
    "#         print(row.result[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# append_dict = {\n",
    "#     'question-id': ['001', '002'],\n",
    "#     'question': ['What is the capital of France? \\\" \\' ðŸ¤— \\' \\\"', 'What is the capital of Germany? </>'],\n",
    "#     'brief-answer': ['Paris', 'Berlin'],\n",
    "# }\n",
    "\n",
    "# # Open the JSONL file in append mode with UTF-8 encoding\n",
    "# # with open(out_file_name, 'ab') as file_handle:\n",
    "# with open('data.jsonl', 'a', encoding='utf-8') as file_handle:\n",
    "#     # Serialize the dictionary to a JSON-formatted string and write it to the file\n",
    "#     for id, q, ba in zip(append_dict['question-id'], append_dict['question'], append_dict['brief-answer']):\n",
    "#         entry = {\n",
    "#             'question-id': id,\n",
    "#             'question': q,\n",
    "#             'brief-answer': ba,\n",
    "#         }\n",
    "#         # json.dump(append_dict, file_handle)\n",
    "#         json.dump(entry, file_handle, ensure_ascii=False)\n",
    "#         # Write a newline character to separate entries\n",
    "#         file_handle.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
