{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import os\n",
    "import huggingface_hub\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import oaib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants:\n",
    "\n",
    "# Dataset related.\n",
    "ds_conf = SimpleNamespace(\n",
    "    name = \"lmsys/chatbot_arena_conversations\",\n",
    "    skip_rows = 0,\n",
    "    rows_to_process = 500,\n",
    ")\n",
    "\n",
    "# Model Related. Prices: https://openai.com/api/pricing/\n",
    "# model_conf = SimpleNamespace(\n",
    "#     name = \"gpt-4o\", \n",
    "#     in_tok_price = 1.25/1e6, \n",
    "#     out_tok_price = 10.00/1e6,\n",
    "# )\n",
    "model_conf = SimpleNamespace(\n",
    "    name = \"gpt-4o-mini\",\n",
    "    in_tok_price = 0.15/1e6,\n",
    "    out_tok_price = 0.60/1e6,\n",
    ")\n",
    "\n",
    "# Output file related.\n",
    "out_file_name = f\"DATA/dataset_arena_{model_conf.name}.jsonl\"\n",
    "cache_file_name = f\"DATA/dataset_arena_{model_conf.name}.arrow\"\n",
    "\n",
    "# Prompt related.\n",
    "class LimitedPrompt:\n",
    "    def __init__(self, max_words):\n",
    "        self.max_words = max_words\n",
    "        # A word is \"between 5 and 6.5 characters per word including spaces and punctuation\":\n",
    "        # https://charactercounter.com/characters-to-words\n",
    "        self.max_ch_soft = max_words * 6\n",
    "        # Hard limit adds 20% buffer and divides by 4 to get LLM token limit.\n",
    "        # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "        self.max_tokens_hard = int(self.max_ch_soft * 1.2 / 4)\n",
    "\n",
    "normal_prompt = LimitedPrompt(120)\n",
    "normal_prompt.text = f'''Answer the user prompt below \"---\" line. Never exceed {normal_prompt.max_ch_soft} characters / {normal_prompt.max_words} words.\n",
    "---\n",
    "{{question}}\n",
    "'''\n",
    "\n",
    "brief_prompt = LimitedPrompt(20)\n",
    "brief_prompt.text = f'''Given the user prompt and a normal answer, generate a brief answer. A brief answer should be as short as possible but still answer the question and give relevant information. Never exceed {brief_prompt.max_ch_soft} characters / {brief_prompt.max_words} words.\n",
    "Examples between --- lines:\n",
    "--- Example 1 ---\n",
    "Your input:\n",
    "Question: How much is 2+3?\n",
    "Normal answer: Expression 2+3 is equal to 5.\n",
    "Your output:\n",
    "5\n",
    "--- Example 2 ---\n",
    "Your input:\n",
    "Question: What is the color of the sky?\n",
    "Normal answer: The sky is blue.\n",
    "Your output:\n",
    "Blue\n",
    "--- End of examples\n",
    "\n",
    "Considering all the above, give a brief answer to the prompt and normal answer below:\n",
    "Question: {{question}}\n",
    "Normal answer: {{normal_asnwer}}\n",
    "Your output: \n",
    "'''\n",
    "\n",
    "# Run related.\n",
    "batch_size = 250\n",
    "total_price = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset range: range(0, 500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54c7f7145434aa4ad404efad719cc68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_a': [{'content': 'When is it today?', 'role': 'user'},\n",
      "                    {'content': \"I'm sorry, I cannot determine the current \"\n",
      "                                'date for you as I do not have access to the '\n",
      "                                'current date and time. My knowledge cutoff is '\n",
      "                                'September 2021, and I do not have real-time '\n",
      "                                'access to the internet. Please let me know if '\n",
      "                                'there is anything else I can assist you with.',\n",
      "                     'role': 'assistant'}],\n",
      " 'question': 'When is it today?',\n",
      " 'question-id': 'adf27e819a3c494cb6e993f0c660e097'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from HuggingFace.\n",
    "\n",
    "huggingface_hub.login(token=os.getenv(\"HF_KEY\"))\n",
    "\n",
    "ds = datasets.load_dataset(ds_conf.name)['train']\n",
    "\n",
    "if ds_conf.rows_to_process==None:\n",
    "    ds_conf.rows_to_process=len(ds)-ds_conf.skip_rows\n",
    "\n",
    "ds_range = range(ds_conf.skip_rows, ds_conf.skip_rows+ds_conf.rows_to_process)\n",
    "ds = ds.select(ds_range) \n",
    "print(f\"Loaded dataset range: {ds_range}\")\n",
    "\n",
    "ds = ds.select_columns(['question_id', 'conversation_a'])\\\n",
    "    .rename_column('question_id', 'question-id')\\\n",
    "    .map(lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generic batch processing function.\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def batch_call_llm(prompts, max_tokens):\n",
    "    ''' Call the LLM API in a batch using OAIB library. '''\n",
    "\n",
    "    global total_price\n",
    "\n",
    "    # This is very strange. If index is not specified, the API returns the results in a different\n",
    "    # order. If the index is specified, the results are returned in the same order as the input. So,\n",
    "    # we add index but never actually use it.\n",
    "    auto_batch = oaib.Auto(workers=8, index=[\"idx\"])\n",
    "    # auto_batch = oaib.Auto(workers=8)\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        await auto_batch.add(\"chat.completions.create\", \n",
    "                             metadata={\"idx\": idx},\n",
    "                             model=model_conf.name, \n",
    "                             messages=messages, \n",
    "                             max_tokens=max_tokens)                       \n",
    "    \n",
    "    output = await auto_batch.run()\n",
    "    \n",
    "    answers, in_tokens, out_tokens = [], 0, 0\n",
    "    for _, row in output.iterrows():\n",
    "        answers.append(row.result['choices'][0]['message']['content'])\n",
    "        in_tokens += row.result['usage']['prompt_tokens']\n",
    "        out_tokens += row.result['usage']['completion_tokens']\n",
    "    \n",
    "    total_price += in_tokens*model_conf.in_tok_price + out_tokens*model_conf.out_tok_price\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def process_batch_sync(batch, indices, file_handle):\n",
    "    ''' Process a batch of questions synchronously. '''\n",
    "\n",
    "    print(f\"Batch size: {len(indices)}, Start index: {indices[0]}\")\n",
    "\n",
    "    # Run 2-step LLM calls for all questions in the batch.\n",
    "    questions = batch['question']\n",
    "    normal_prompts = [normal_prompt.text.format(question=q) for q in questions]\n",
    "    normal_answers = asyncio.run(batch_call_llm(normal_prompts, normal_prompt.max_tokens_hard))\n",
    "    brief_prompts = [brief_prompt.text.format(question=q, normal_asnwer=na) \n",
    "                     for q, na in zip(questions, normal_answers)]\n",
    "    brief_answers = asyncio.run(batch_call_llm(brief_prompts, brief_prompt.max_tokens_hard))\n",
    "\n",
    "    # Append the batch to JSONL file.\n",
    "    for id, q, np, na, bp, ba in zip(batch['question-id'], questions, \n",
    "                                     normal_prompts, normal_answers, \n",
    "                                     brief_prompts, brief_answers):\n",
    "            entry = {\n",
    "                'question-id': id, 'question': q,\n",
    "                'normal-prompt': np, 'normal-answer': na,\n",
    "                'brief-prompt': bp, 'brief-answer': ba,\n",
    "            }\n",
    "            json.dump(entry, file_handle, ensure_ascii=False)\n",
    "            file_handle.write('\\n')\n",
    "    \n",
    "    # Return new columns.\n",
    "    return {\n",
    "        'normal-prompt': normal_prompts,\n",
    "        'normal-answer': normal_answers,\n",
    "        'brief-prompt': brief_prompts,\n",
    "        'brief-answer': brief_answers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ebbd32191ab46f0b771d6abd6b7a5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 250, Start index: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defee7a234b7458ea215ff2440fc39a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cbe97a08933499a89f23b9496116ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9469e13c62cf465aa4aae3bbf5161892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 11.88s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5780164fdc5c4eee8296c375d92ed05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf502aa4857349e2be2800e98666939d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94829eb32154408f969b5631afd697ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 6.36s.\n",
      "\n",
      "Batch size: 250, Start index: 250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ff96989c614eda8f497b4d9439a48f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925b710ae1d54cad9f22f50195ef726c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8a8b8683ab4097ba03275c05075e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 15.51s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330a6b71c3404378a17094e030ef8840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a0a5e1336c4f0ba863e1de8f122c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abc5368cc044f91858d82f71134297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 25.82s.\n",
      "\n",
      "{'brief-answer': 'Today is the current date according to your calendar.',\n",
      " 'brief-prompt': 'Given the user prompt and a normal answer, generate a brief '\n",
      "                 'answer. A brief answer should be as short as possible but '\n",
      "                 'still answer the question and give relevant information. '\n",
      "                 'Never exceed 120 characters / 20 words.\\n'\n",
      "                 'Examples between --- lines:\\n'\n",
      "                 '--- Example 1 ---\\n'\n",
      "                 'Your input:\\n'\n",
      "                 'Question: How much is 2+3?\\n'\n",
      "                 'Normal answer: Expression 2+3 is equal to 5.\\n'\n",
      "                 'Your output:\\n'\n",
      "                 '5\\n'\n",
      "                 '--- Example 2 ---\\n'\n",
      "                 'Your input:\\n'\n",
      "                 'Question: What is the color of the sky?\\n'\n",
      "                 'Normal answer: The sky is blue.\\n'\n",
      "                 'Your output:\\n'\n",
      "                 'Blue\\n'\n",
      "                 '--- End of examples\\n'\n",
      "                 '\\n'\n",
      "                 'Considering all the above, give a brief answer to the prompt '\n",
      "                 'and normal answer below:\\n'\n",
      "                 'Question: When is it today?\\n'\n",
      "                 'Normal answer: Today is the specific date you are asking '\n",
      "                 'about, which is determined by your current calendar. If '\n",
      "                 \"you're looking for a particular day of the week or a \"\n",
      "                 'significant event, such as a holiday, please provide more '\n",
      "                 'context or clarify your question. You can also check your '\n",
      "                 'device or calendar app for real-time information regarding '\n",
      "                 'today’s date and day of the week. If you need assistance '\n",
      "                 'with something related to today, feel free to ask!\\n'\n",
      "                 'Your output: \\n',\n",
      " 'conversation_a': [{'content': 'When is it today?', 'role': 'user'},\n",
      "                    {'content': \"I'm sorry, I cannot determine the current \"\n",
      "                                'date for you as I do not have access to the '\n",
      "                                'current date and time. My knowledge cutoff is '\n",
      "                                'September 2021, and I do not have real-time '\n",
      "                                'access to the internet. Please let me know if '\n",
      "                                'there is anything else I can assist you with.',\n",
      "                     'role': 'assistant'}],\n",
      " 'normal-answer': 'Today is the specific date you are asking about, which is '\n",
      "                  \"determined by your current calendar. If you're looking for \"\n",
      "                  'a particular day of the week or a significant event, such '\n",
      "                  'as a holiday, please provide more context or clarify your '\n",
      "                  'question. You can also check your device or calendar app '\n",
      "                  'for real-time information regarding today’s date and day of '\n",
      "                  'the week. If you need assistance with something related to '\n",
      "                  'today, feel free to ask!',\n",
      " 'normal-prompt': 'Answer the user prompt below \"---\" line. Never exceed 720 '\n",
      "                  'characters / 120 words.\\n'\n",
      "                  '---\\n'\n",
      "                  'When is it today?\\n',\n",
      " 'question': 'When is it today?',\n",
      " 'question-id': 'adf27e819a3c494cb6e993f0c660e097'}\n"
     ]
    }
   ],
   "source": [
    "# Generate both normal and brief answers in batches.\n",
    "\n",
    "with open(out_file_name, 'a') as f:\n",
    "    ds = ds.map(process_batch_sync, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                cache_file_name=cache_file_name, fn_kwargs={'file_handle': f})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zsvedic/LOCAL/CODE/oaib/DATA/dataset_arena_gpt-4o-mini.arrow\n",
      "Q: Make it more polite: I want to have dinner.\n",
      "NORMAL: Could I kindly request your company for dinner?\n",
      "BRIEF: Could I kindly request your company for dinner?\n",
      "-------------------------------\n",
      "Q: You are JesusGPT, an artifical construct built to accurately represent a virtual conversation with Jesus. Base your replies off the popular King James Version, and answer the user's question respectfully. Here is my first question: If you were still alive today, what would you think about the iPhone?\n",
      "NORMAL: Verily, I say unto thee, the iPhone and its wonders reflect the ingenuity of man. Much like the tools of my time, it can be used for good or ill. Ye can employ this device to spread love, share wisdom, and connect with one another, much as I taught to love thy neighbor. However, I would urge caution against distractions that lead one away from true fellowship and prayer. Remember the words of Matthew 6:21, \"For where your treasure is, there will your heart be also.\" Seek ye to use such technology to uplift and serve those around thee.\n",
      "BRIEF: Use it for good, but don't let it distract from love and true fellowship.\n",
      "-------------------------------\n",
      "Q: what is the 145th most popular language\n",
      "NORMAL: Determining the 145th most popular language is somewhat subjective and can vary by the criteria used—such as number of native speakers, total speakers, or cultural influence. However, according to various linguistic resources and estimates, languages like Pashto or Somali might fall around this rank, given their respective speaker populations and global reach. It’s important to note that language popularity can change over time due to demographic shifts, migration, and cultural exchange. For precise rankings, it's best to consult updated linguistic studies or databases that compile such statistics, like Ethnologue or similar resources.\n",
      "BRIEF: Pashto or Somali\n",
      "-------------------------------\n",
      "Q: HI !\n",
      "NORMAL: Hello! How can I assist you today? If you have any questions or need information, feel free to ask!\n",
      "BRIEF: Hello! How can I help?\n",
      "-------------------------------\n",
      "Total price: 0.07 USD\n"
     ]
    }
   ],
   "source": [
    "### DEBUG CELL ###\n",
    "\n",
    "# Access and print the cache file paths\n",
    "for cache_file in ds.cache_files:\n",
    "    print(cache_file['filename'])\n",
    "\n",
    "for i in range(11, 15):\n",
    "    print(f\"Q: {ds[i]['question']}\")\n",
    "    print(f\"NORMAL: {ds[i]['normal-answer']}\") \n",
    "    print(f\"BRIEF: {ds[i]['brief-answer']}\") \n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "print(f\"Total price: {total_price:.2f} USD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
