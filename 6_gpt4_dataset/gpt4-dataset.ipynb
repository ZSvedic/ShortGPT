{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import os\n",
    "import huggingface_hub\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import oaib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants:\n",
    "\n",
    "# Dataset related.\n",
    "ds_conf = SimpleNamespace(\n",
    "    name = \"lmsys/chatbot_arena_conversations\",\n",
    "    skip_rows = 27000,\n",
    "    rows_to_process = None, # None for all rows.\n",
    ")\n",
    "\n",
    "# Model Related. Prices: https://openai.com/api/pricing/\n",
    "model_conf = SimpleNamespace(\n",
    "    name = \"gpt-4o\", \n",
    "    in_tok_price = 1.25/1e6, \n",
    "    out_tok_price = 10.00/1e6,\n",
    ")\n",
    "# model_conf = SimpleNamespace(\n",
    "#     name = \"gpt-4o-mini\",\n",
    "#     in_tok_price = 0.15/1e6,\n",
    "#     out_tok_price = 0.60/1e6,\n",
    "# )\n",
    "\n",
    "# Output file related.\n",
    "out_file_name = f\"DATA/dataset_arena_{model_conf.name}.jsonl\"\n",
    "cache_file_name = f\"DATA/dataset_arena_{model_conf.name}.arrow\"\n",
    "\n",
    "# Prompt related.\n",
    "class LimitedPrompt:\n",
    "    def __init__(self, max_words):\n",
    "        self.max_words = max_words\n",
    "        # A word is \"between 5 and 6.5 characters per word including spaces and punctuation\":\n",
    "        # https://charactercounter.com/characters-to-words\n",
    "        self.max_ch_soft = max_words * 6\n",
    "        # Hard limit adds 20% buffer and divides by 4 to get LLM token limit.\n",
    "        # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "        self.max_tokens_hard = int(self.max_ch_soft * 1.2 / 4)\n",
    "\n",
    "normal_prompt = LimitedPrompt(120)\n",
    "normal_prompt.text = f'''Answer the user prompt below \"---\" line. Never exceed {normal_prompt.max_ch_soft} characters / {normal_prompt.max_words} words.\n",
    "---\n",
    "{{question}}\n",
    "'''\n",
    "\n",
    "brief_prompt = LimitedPrompt(20)\n",
    "brief_prompt.text = f'''Given the user prompt and a normal answer, generate a brief answer. A brief answer should be as short as possible but still answer the question and give relevant information. Never exceed {brief_prompt.max_ch_soft} characters / {brief_prompt.max_words} words.\n",
    "Examples between --- lines:\n",
    "--- Example 1 ---\n",
    "Your input:\n",
    "Question: How much is 2+3?\n",
    "Normal answer: Expression 2+3 is equal to 5.\n",
    "Your output:\n",
    "5\n",
    "--- Example 2 ---\n",
    "Your input:\n",
    "Question: What is the color of the sky?\n",
    "Normal answer: The sky is blue.\n",
    "Your output:\n",
    "Blue\n",
    "--- End of examples\n",
    "\n",
    "Considering all the above, give a brief answer to the prompt and normal answer below:\n",
    "Question: {{question}}\n",
    "Normal answer: {{normal_asnwer}}\n",
    "Your output: \n",
    "'''\n",
    "\n",
    "# Run related.\n",
    "batch_size = 500\n",
    "total_price = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/zel/.cache/huggingface/token\n",
      "Login successful\n",
      "Loaded dataset range: range(27000, 33000)\n",
      "{'conversation_a': [{'content': 'make an original tongue twister with black '\n",
      "                                'metal aesthetics',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '\"Faster than a black metal lightning, more '\n",
      "                                'evil than a demonic riff, stronger than an '\n",
      "                                'abyssal howl, and more intense than a blast '\n",
      "                                'beat, the darkness shall consume all in its '\n",
      "                                'wake, leaving only ash and embers in its '\n",
      "                                'path!\"',\n",
      "                     'role': 'assistant'}],\n",
      " 'question': 'make an original tongue twister with black metal aesthetics',\n",
      " 'question-id': '7f3a466ac1e74717bcf5767900c569db'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from HuggingFace.\n",
    "\n",
    "huggingface_hub.login(token=os.getenv(\"HF_KEY\"))\n",
    "\n",
    "ds = datasets.load_dataset(ds_conf.name)['train']\n",
    "\n",
    "if ds_conf.rows_to_process==None:\n",
    "    ds_conf.rows_to_process=len(ds)-ds_conf.skip_rows\n",
    "\n",
    "ds_range = range(ds_conf.skip_rows, ds_conf.skip_rows+ds_conf.rows_to_process)\n",
    "ds = ds.select(ds_range) \n",
    "print(f\"Loaded dataset range: {ds_range}\")\n",
    "\n",
    "ds = ds.select_columns(['question_id', 'conversation_a'])\\\n",
    "    .rename_column('question_id', 'question-id')\\\n",
    "    .map(lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generic batch processing function.\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def batch_call_llm(prompts, max_tokens):\n",
    "    ''' Call the LLM API in a batch using OAIB library. '''\n",
    "\n",
    "    global total_price\n",
    "\n",
    "    # This is very strange. If index is not specified, the API returns the results in a different\n",
    "    # order. If the index is specified, the results are returned in the same order as the input. So,\n",
    "    # we add index but never actually use it.\n",
    "    auto_batch = oaib.Auto(workers=8, index=[\"idx\"])\n",
    "    # auto_batch = oaib.Auto(workers=8)\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        await auto_batch.add(\"chat.completions.create\", \n",
    "                             metadata={\"idx\": idx+1000}, # Add 1000 for debugging purposes.\n",
    "                             model=model_conf.name, \n",
    "                             messages=messages, \n",
    "                             max_tokens=max_tokens)                       \n",
    "    \n",
    "    output = await auto_batch.run()\n",
    "\n",
    "    if len(output)!=len(prompts):\n",
    "        print(f\"MISSING ANSWERS ISSUE: len(output)={len(output)}, len(prompts)={len(prompts)}\")\n",
    "    \n",
    "    answers = [\"\"]*batch_size\n",
    "    in_tokens, out_tokens = 0, 0\n",
    "    for _, row in output.iterrows():\n",
    "        idx = row.name-1000 # ChatGPT: \"The Name attribute of a Series in pandas always corresponds to the index value of that row in the DataFrame.\"\n",
    "        answers[idx] = row.result['choices'][0]['message']['content']\n",
    "        in_tokens += row.result['usage']['prompt_tokens']\n",
    "        out_tokens += row.result['usage']['completion_tokens']\n",
    "    \n",
    "    total_price += in_tokens*model_conf.in_tok_price + out_tokens*model_conf.out_tok_price\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def process_batch_sync(batch, indices, file_handle):\n",
    "    ''' Process a batch of questions synchronously. '''\n",
    "\n",
    "    print(f\"Batch size: {len(indices)}, Start index: {indices[0]}\")\n",
    "\n",
    "    # Run 2-step LLM calls for all questions in the batch.\n",
    "    retry = 0\n",
    "    while retry<2:\n",
    "        questions = batch['question']\n",
    "        normal_prompts = [normal_prompt.text.format(question=q) for q in questions]\n",
    "        normal_answers = asyncio.run(batch_call_llm(normal_prompts, normal_prompt.max_tokens_hard))\n",
    "        brief_prompts = [brief_prompt.text.format(question=q, normal_asnwer=na) \n",
    "                        for q, na in zip(questions, normal_answers)]\n",
    "        brief_answers = asyncio.run(batch_call_llm(brief_prompts, brief_prompt.max_tokens_hard))\n",
    "        if len(questions)==len(normal_answers)==len(brief_answers)==batch_size:\n",
    "            break\n",
    "        else:\n",
    "            print(f\"ANSWERS MISSING, RETRYING: len(questions)={len(questions)}, \" + \n",
    "                   f\"len(normal_answers)={len(normal_answers)}, len(brief_answers)={len(brief_answers)}\") \n",
    "            retry += 1\n",
    "\n",
    "    # Append the batch to JSONL file.\n",
    "    for id, q, np, na, bp, ba in zip(batch['question-id'], questions, \n",
    "                                     normal_prompts, normal_answers, \n",
    "                                     brief_prompts, brief_answers):\n",
    "            entry = {\n",
    "                'question-id': id, 'question': q,\n",
    "                'normal-prompt': np, 'normal-answer': na,\n",
    "                'brief-prompt': bp, 'brief-answer': ba,\n",
    "            }\n",
    "            json.dump(entry, file_handle, ensure_ascii=False)\n",
    "            file_handle.write('\\n')\n",
    "    \n",
    "    # Return new columns.\n",
    "    return {\n",
    "        'normal-prompt': normal_prompts,\n",
    "        'normal-answer': normal_answers,\n",
    "        'brief-prompt': brief_prompts,\n",
    "        'brief-answer': brief_answers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce0af8ea2844c75bc614d6d2c37bac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 500, Start index: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f55efaf1c04afdaa97107de0f4c718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "152b62cdf6d54551be526c1ab5c5b1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df629c02adb4041b7f22ff7046dfeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 19.57s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37bfc1562804763a453cc5e9879ccb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf237ae9b67d4297be5263e8cb44980a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dc3d60e9c443edbf154c58bff24600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 27.50s.\n",
      "\n",
      "Batch size: 500, Start index: 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5101b9c325e944f284be5613f58fa585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e51ad07eb03441ba30c26b4926bdc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf6bf81ca204f1c95552e1094349088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 17.69s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4633eee4064b55bf3185e43419b23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3190acca6a6945e78e654eeda43493a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d2b76a6cbf419a8c6b7fb60efdef6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 26.60s.\n",
      "\n",
      "Batch size: 500, Start index: 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0553497ae043d98cba97959c24bf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d862ff06464742a4dffe40a28571be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fc7681bd8e4d819d52b275eac3dbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 22.97s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682fb412a0c14a0c9008df2fa120527b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b390adb6de494e949c5abe3799076ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4102cb3ae3d84cbb97821bfe60a689e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 28.18s.\n",
      "\n",
      "Batch size: 500, Start index: 1500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf31a42ff044e9aa920664900ee59d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b513e03bdf274e50b4ac4714080c9941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bac84b577243308d9141e04ddc99f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 15.25s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8648938294a64147a88cdb8c83a0a59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e8a7383c7448608aad89eab78e3266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83699721eef94c20a5be5000a6b1a953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 61.40s.\n",
      "\n",
      "MISSING ANSWERS ISSUE: len(output)=499, len(prompts)=500\n",
      "endpoint                                chat.completions.create\n",
      "model                                                    gpt-4o\n",
      "messages      [{'role': 'user', 'content': 'Given the user p...\n",
      "max_tokens                                                   36\n",
      "result        {'id': 'chatcmpl-AgVbYazfCPRj5mSF7dSkdSjC1h5Cs...\n",
      "Name: 1067, dtype: object\n",
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'audio': None,\n",
      "                          'content': 'Autobiography: full life story; memoir: '\n",
      "                                     'specific themes/events, introspective.',\n",
      "                          'function_call': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None}}],\n",
      " 'created': 1734694628,\n",
      " 'id': 'chatcmpl-AgVbYazfCPRj5mSF7dSkdSjC1h5Cs',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'fp_d28bcae782',\n",
      " 'usage': {'completion_tokens': 18,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 274,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 292}}\n",
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'audio': None,\n",
      "                          'content': 'Reduce plastic, improve waste, innovate, '\n",
      "                                     'educate, enforce regulations, and '\n",
      "                                     'research for solutions.',\n",
      "                          'function_call': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None}}],\n",
      " 'created': 1734694628,\n",
      " 'id': 'chatcmpl-AgVbYRo0jkFaKVaASRwKbOko8HYzu',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'fp_d28bcae782',\n",
      " 'usage': {'completion_tokens': 19,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 317,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 336}}\n",
      "[{'content': 'Given the user prompt and a normal answer, generate a brief '\n",
      "             'answer. A brief answer should be as short as possible but still '\n",
      "             'answer the question and give relevant information. Never exceed '\n",
      "             '120 characters / 20 words.\\n'\n",
      "             'Examples between --- lines:\\n'\n",
      "             '--- Example 1 ---\\n'\n",
      "             'Your input:\\n'\n",
      "             'Question: How much is 2+3?\\n'\n",
      "             'Normal answer: Expression 2+3 is equal to 5.\\n'\n",
      "             'Your output:\\n'\n",
      "             '5\\n'\n",
      "             '--- Example 2 ---\\n'\n",
      "             'Your input:\\n'\n",
      "             'Question: What is the color of the sky?\\n'\n",
      "             'Normal answer: The sky is blue.\\n'\n",
      "             'Your output:\\n'\n",
      "             'Blue\\n'\n",
      "             '--- End of examples\\n'\n",
      "             '\\n'\n",
      "             'Considering all the above, give a brief answer to the prompt and '\n",
      "             'normal answer below:\\n'\n",
      "             'Question: How to solve the microplastic problem\\n'\n",
      "             'Normal answer: Addressing the microplastic problem requires a '\n",
      "             'multi-faceted approach:\\n'\n",
      "             '\\n'\n",
      "             '1. **Reduce Plastic Production:** Decrease reliance on '\n",
      "             'single-use plastics and promote sustainable alternatives.\\n'\n",
      "             '   \\n'\n",
      "             '2. **Improve Waste Management:** Enhance recycling systems and '\n",
      "             'promote proper disposal methods to prevent plastic leakage into '\n",
      "             'ecosystems.\\n'\n",
      "             '\\n'\n",
      "             '3. **Innovate Materials:** Develop biodegradable or eco-friendly '\n",
      "             'plastics that break down more easily and safely.\\n'\n",
      "             '\\n'\n",
      "             '4. **Education and Awareness:** Increase public awareness about '\n",
      "             'the effects of microplastics and encourage responsible consumer '\n",
      "             'behavior.\\n'\n",
      "             '\\n'\n",
      "             '5. **Regulations and Policies:** Enforce stricter regulations on '\n",
      "             'plastic production, use, and disposal.\\n'\n",
      "             '\\n'\n",
      "             '6. **Research and Monitoring:** Invest in research to understand '\n",
      "             \"microplastics' impact and track pollution sources.\\n\"\n",
      "             '\\n'\n",
      "             'Collaboration among governments, industries, and communities is '\n",
      "             'crucial.\\n'\n",
      "             'Your output: \\n',\n",
      "  'role': 'user'}]\n",
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'audio': None,\n",
      "                          'content': 'Reduce plastic, improve waste, innovate, '\n",
      "                                     'educate, enforce regulations, and '\n",
      "                                     'research for solutions.',\n",
      "                          'function_call': None,\n",
      "                          'refusal': None,\n",
      "                          'role': 'assistant',\n",
      "                          'tool_calls': None}}],\n",
      " 'created': 1734694628,\n",
      " 'id': 'chatcmpl-AgVbYRo0jkFaKVaASRwKbOko8HYzu',\n",
      " 'model': 'gpt-4o-2024-08-06',\n",
      " 'object': 'chat.completion',\n",
      " 'service_tier': None,\n",
      " 'system_fingerprint': 'fp_d28bcae782',\n",
      " 'usage': {'completion_tokens': 19,\n",
      "           'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
      "                                         'audio_tokens': 0,\n",
      "                                         'reasoning_tokens': 0,\n",
      "                                         'rejected_prediction_tokens': 0},\n",
      "           'prompt_tokens': 317,\n",
      "           'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0},\n",
      "           'total_tokens': 336}}\n",
      "[{'finish_reason': 'stop',\n",
      "  'index': 0,\n",
      "  'logprobs': None,\n",
      "  'message': {'audio': None,\n",
      "              'content': 'Reduce plastic, improve waste, innovate, educate, '\n",
      "                         'enforce regulations, and research for solutions.',\n",
      "              'function_call': None,\n",
      "              'refusal': None,\n",
      "              'role': 'assistant',\n",
      "              'tool_calls': None}}]\n",
      "[{'finish_reason': 'stop',\n",
      "  'index': 0,\n",
      "  'logprobs': None,\n",
      "  'message': {'audio': None,\n",
      "              'content': 'Autobiography: full life story; memoir: specific '\n",
      "                         'themes/events, introspective.',\n",
      "              'function_call': None,\n",
      "              'refusal': None,\n",
      "              'role': 'assistant',\n",
      "              'tool_calls': None}}]\n",
      "1067\n",
      "[{'content': 'Given the user prompt and a normal answer, generate a brief '\n",
      "             'answer. A brief answer should be as short as possible but still '\n",
      "             'answer the question and give relevant information. Never exceed '\n",
      "             '120 characters / 20 words.\\n'\n",
      "             'Examples between --- lines:\\n'\n",
      "             '--- Example 1 ---\\n'\n",
      "             'Your input:\\n'\n",
      "             'Question: How much is 2+3?\\n'\n",
      "             'Normal answer: Expression 2+3 is equal to 5.\\n'\n",
      "             'Your output:\\n'\n",
      "             '5\\n'\n",
      "             '--- Example 2 ---\\n'\n",
      "             'Your input:\\n'\n",
      "             'Question: What is the color of the sky?\\n'\n",
      "             'Normal answer: The sky is blue.\\n'\n",
      "             'Your output:\\n'\n",
      "             'Blue\\n'\n",
      "             '--- End of examples\\n'\n",
      "             '\\n'\n",
      "             'Considering all the above, give a brief answer to the prompt and '\n",
      "             'normal answer below:\\n'\n",
      "             \"Question: What's the difference between an autobiography and \"\n",
      "             'memoirs?\\n'\n",
      "             'Normal answer: An autobiography is a comprehensive account of a '\n",
      "             \"person's life written by that person, typically covering their \"\n",
      "             'life from birth to the time of writing and written in '\n",
      "             'chronological order. It includes a factual recounting of events '\n",
      "             'and personal experiences, often emphasizing chronological '\n",
      "             'detail. On the other hand, memoirs focus on specific themes, '\n",
      "             \"events, or periods in the author's life rather than a complete \"\n",
      "             'biography. Memoirs tend to be more introspective and reflective, '\n",
      "             \"providing insight into the author's thoughts and feelings about \"\n",
      "             'particular experiences. They often emphasize emotional truth '\n",
      "             'over strict adherence to factual accuracy.\\n'\n",
      "             'Your output: \\n',\n",
      "  'role': 'user'}]\n",
      "('Given the user prompt and a normal answer, generate a brief answer. A brief '\n",
      " 'answer should be as short as possible but still answer the question and give '\n",
      " 'relevant information. Never exceed 120 characters / 20 words.\\n'\n",
      " 'Examples between --- lines:\\n'\n",
      " '--- Example 1 ---\\n'\n",
      " 'Your input:\\n'\n",
      " 'Question: How much is 2+3?\\n'\n",
      " 'Normal answer: Expression 2+3 is equal to 5.\\n'\n",
      " 'Your output:\\n'\n",
      " '5\\n'\n",
      " '--- Example 2 ---\\n'\n",
      " 'Your input:\\n'\n",
      " 'Question: What is the color of the sky?\\n'\n",
      " 'Normal answer: The sky is blue.\\n'\n",
      " 'Your output:\\n'\n",
      " 'Blue\\n'\n",
      " '--- End of examples\\n'\n",
      " '\\n'\n",
      " 'Considering all the above, give a brief answer to the prompt and normal '\n",
      " 'answer below:\\n'\n",
      " 'Question: show me and explain how to rewrite this lua script from my yugioh '\n",
      " 'monster card to the new description of what it should do, previously it '\n",
      " 'added a specific card from the deck to the hand, hence the use of the '\n",
      " 'iscode() function, now it returns all fusions from the graveyard that have '\n",
      " 'been used as fusion materials back to the extra deck, the activation cost '\n",
      " 'remains the same, use c:IsType(TYPE_FUSION) to check if it is a fusion and '\n",
      " '(c:GetReason()&0x8)==0x8 to check if it was used as fusion material. my code '\n",
      " 'for you to rewrite:\\n'\n",
      " '--Poyomon\\n'\n",
      " 'local s,id,o=GetID()\\n'\n",
      " 'function s.initial_effect(c)\\n'\n",
      " '    --search\\n'\n",
      " '    local e1=Effect.CreateEffect(c)\\n'\n",
      " '    e1:SetDescription(aux.Stringid(id,0))\\n'\n",
      " '    e1:SetCategory(CATEGORY_TOHAND+CATEGORY_SEARCH)\\n'\n",
      " '    e1:SetType(EFFECT_TYPE_IGNITION)\\n'\n",
      " '    e1:SetRange(LOCATION_HAND)\\n'\n",
      " '    e1:SetCost(s.cost)\\n'\n",
      " '    e1:SetTarget(s.target)\\n'\n",
      " '    e1:SetOperation(s.operation)\\n'\n",
      " '    c:RegisterEffect(e1)\\n'\n",
      " 'end\\n'\n",
      " 'function s.cost(e,tp,eg,ep,ev,re,r,rp,chk)\\n'\n",
      " '    local c=e:GetHandler()\\n'\n",
      " '    if chk==0 then return c:IsAbleToGraveAsCost() and c:IsDiscardable() end\\n'\n",
      " '    Duel.SendtoGrave(c,REASON_COST+REASON_DISCARD)\\n'\n",
      " 'end\\n'\n",
      " 'function s.filter(c)\\n'\n",
      " '    return c:IsCode(37501101) and c:IsAbleToHand()\\n'\n",
      " 'end\\n'\n",
      " 'function s.target(e,tp,eg,ep,ev,re,r,rp,chk)\\n'\n",
      " '    if chk==0 then return '\n",
      " 'Duel.IsExistingMatchingCard(s.filter,tp,LOCATION_DECK,0,1,nil) end\\n'\n",
      " '    Duel.SetOperationInfo(0,CATEGORY_TOHAND,nil,1,tp,LOCATION_DECK)\\n'\n",
      " 'end\\n'\n",
      " 'function s.operation(e,tp,eg,ep,ev,re,r,rp,chk)\\n'\n",
      " '    Duel.Hint(HINT_SELECTMSG,tp,HINTMSG_ATOHAND)\\n'\n",
      " '    local g=Duel.SelectMatchingCard(tp,s.filter,tp,LOCATION_DECK,0,1,1,nil)\\n'\n",
      " '    if #g>0 then\\n'\n",
      " '        Duel.SendtoHand(g,nil,REASON_EFFECT)\\n'\n",
      " '        Duel.ConfirmCards(1-tp,g)\\n'\n",
      " '    end\\n'\n",
      " 'end\\n'\n",
      " 'Normal answer: ```lua\\n'\n",
      " '--Poyomon\\n'\n",
      " 'local s,id,o=GetID()\\n'\n",
      " 'function s.initial_effect(c)\\n'\n",
      " '    --return fusion materials\\n'\n",
      " '    local e1=Effect.CreateEffect(c)\\n'\n",
      " '    e1:SetDescription(aux.Stringid(id,0))\\n'\n",
      " '    e1:SetCategory(CATEGORY_TODECK)\\n'\n",
      " '    e1:SetType(EFFECT_TYPE_IGNITION)\\n'\n",
      " '    e1:SetRange(LOCATION_HAND)\\n'\n",
      " '    e1:SetCost(s.cost)\\n'\n",
      " '    e1:SetTarget(s.target)\\n'\n",
      " '    e1:SetOperation(s.operation)\\n'\n",
      " '    c:RegisterEffect(e1)\\n'\n",
      " 'end\\n'\n",
      " 'function s.cost(e,tp,eg,ep,ev,re,r,rp,chk)\\n'\n",
      " '    local c=e:GetHandler()\\n'\n",
      " '    if chk==0 then return c:IsAbleToGraveAsCost() and c:IsDiscardable() end\\n'\n",
      " '    Duel.SendtoGrave(c,REASON_COST+REASON_DISCARD)\\n'\n",
      " 'end\\n'\n",
      " 'function s.filter(c)\\n'\n",
      " '    return c:IsType(TYPE_FUSION) and (c:GetReason()&0x8)==0x8 and '\n",
      " 'c:IsAbleTo\\n'\n",
      " 'Your output: \\n')\n",
      "Batch size: 500, Start index: 2000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c04f929c1014153a09221de8d5ab04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d16bd32a81046c29aa9100ee1904a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3db3c2695f14cc59b8fd2d074ef5f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 27.08s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6d20f9ad5247489f399f97f2a18bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6de7633b682462cb82056ed304c3ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450e4bad3efa4e9b9f990eeaee82e5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 24.21s.\n",
      "\n",
      "Batch size: 500, Start index: 2500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da8a0a616fc45bb80b4a655839b8593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412b259c92cd4800bda4393d5ee1afde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5249836aac41d08c1f3508c62d0b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 18.96s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235b00d07b9b4525aaeee9b52f2068b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16f8b5762e94058905ab73542fa8f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a501cf8848a43ffbab92609a5d361ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 26.01s.\n",
      "\n",
      "Batch size: 500, Start index: 3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdc761dc6564efbb1d269ae3c3c829e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1dc64a3052474a8a05b07932af99cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13449955ffb64384b328a8daeb3295d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 19.17s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c21e3d4ad041859cc29baaaa8d86f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ee4c04e221407e8e74ae9801dc65d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d8bc73d404449ebea091a7ce739b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 26.91s.\n",
      "\n",
      "Batch size: 500, Start index: 3500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a013af009afa408b97d0afee163a2284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48cd4dcaef2428a91d22c868959161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecb1fe1476a48e5935dc67103388626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 63.42s.\n",
      "\n",
      "MISSING ANSWERS ISSUE: len(output)=499, len(prompts)=500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badee7c022db47cabfd153d9b772f807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56296438860b43bb9c9b4e5125bbfe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "772c0d93415c4cf6b8762f31d8d2c64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 34.08s.\n",
      "\n",
      "Batch size: 500, Start index: 4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e06302db13f4f9793e6d20d8ff96310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26e8ade926a45fcbc8c62e86af110f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21f53f494d54432b89a9ae747474712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 18.53s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f8e255a9e4424d92a1ea20c15350a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc589554b784a8bb23ed3c7a8d445c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b696f960fcd4e518bcc23af449f32b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 13.24s.\n",
      "\n",
      "Batch size: 500, Start index: 4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d0816a459f47d8a1c8b3969524a438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ea28f76ad9450fba10e9ff070fc95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c1771445e14522beb51c5934eff432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 18.21s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef142e737fb41478364c04125f775da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618f870809944248ac8f0322c4ddad4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621410ae0a7e4ae8a8b8cf4d16b3f0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 29.97s.\n",
      "\n",
      "Batch size: 500, Start index: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0661d3cb267745a388268a95e416d09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca3c353f1a44543af6e0a956bf4cabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ca55710cff441fabcd6afb844807a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 18.53s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bdefc2a8b24a1faf642f44204e5b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1348374f8eb84b019586cb9f01f31292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2c3da07c1945bd946d14f518e0f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 23.79s.\n",
      "\n",
      "Batch size: 500, Start index: 5500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d53852ac3e490091c149ad252f26bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228b8320f44a4347af94273d949b1757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aadae5c41d41269ecc8c3cc68b2617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 25.19s.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe66305d346472c96cd3427c2aae1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4747927b9f7d4879882e7b6295cb0ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RPM:   0%|          | 0/500"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956ccd9105cb446da1e9c684165413b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TPM:   0%|          | 0/10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run took 27.48s.\n",
      "\n",
      "{'brief-answer': 'Bleak black bats bask beneath bloodred banners, blasting '\n",
      "                 'black metal before midnight’s mist.',\n",
      " 'brief-prompt': 'Given the user prompt and a normal answer, generate a brief '\n",
      "                 'answer. A brief answer should be as short as possible but '\n",
      "                 'still answer the question and give relevant information. '\n",
      "                 'Never exceed 120 characters / 20 words.\\n'\n",
      "                 'Examples between --- lines:\\n'\n",
      "                 '--- Example 1 ---\\n'\n",
      "                 'Your input:\\n'\n",
      "                 'Question: How much is 2+3?\\n'\n",
      "                 'Normal answer: Expression 2+3 is equal to 5.\\n'\n",
      "                 'Your output:\\n'\n",
      "                 '5\\n'\n",
      "                 '--- Example 2 ---\\n'\n",
      "                 'Your input:\\n'\n",
      "                 'Question: What is the color of the sky?\\n'\n",
      "                 'Normal answer: The sky is blue.\\n'\n",
      "                 'Your output:\\n'\n",
      "                 'Blue\\n'\n",
      "                 '--- End of examples\\n'\n",
      "                 '\\n'\n",
      "                 'Considering all the above, give a brief answer to the prompt '\n",
      "                 'and normal answer below:\\n'\n",
      "                 'Question: make an original tongue twister with black metal '\n",
      "                 'aesthetics\\n'\n",
      "                 'Normal answer: Bleak black bats bask beneath bloodred '\n",
      "                 'banners, blasting blistering black metal melodies before '\n",
      "                 'midnight’s malevolent mist.\\n'\n",
      "                 'Your output: \\n',\n",
      " 'conversation_a': [{'content': 'make an original tongue twister with black '\n",
      "                                'metal aesthetics',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '\"Faster than a black metal lightning, more '\n",
      "                                'evil than a demonic riff, stronger than an '\n",
      "                                'abyssal howl, and more intense than a blast '\n",
      "                                'beat, the darkness shall consume all in its '\n",
      "                                'wake, leaving only ash and embers in its '\n",
      "                                'path!\"',\n",
      "                     'role': 'assistant'}],\n",
      " 'normal-answer': 'Bleak black bats bask beneath bloodred banners, blasting '\n",
      "                  'blistering black metal melodies before midnight’s '\n",
      "                  'malevolent mist.',\n",
      " 'normal-prompt': 'Answer the user prompt below \"---\" line. Never exceed 720 '\n",
      "                  'characters / 120 words.\\n'\n",
      "                  '---\\n'\n",
      "                  'make an original tongue twister with black metal '\n",
      "                  'aesthetics\\n',\n",
      " 'question': 'make an original tongue twister with black metal aesthetics',\n",
      " 'question-id': '7f3a466ac1e74717bcf5767900c569db'}\n"
     ]
    }
   ],
   "source": [
    "# Generate both normal and brief answers in batches.\n",
    "\n",
    "with open(out_file_name, 'a') as f:\n",
    "    ds = ds.map(process_batch_sync, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                cache_file_name=cache_file_name, fn_kwargs={'file_handle': f})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset len: 33000\n",
      "Len after removing duplicate questions: 26968\n",
      "Len after removing bad data: 25490\n",
      "Columns after renaming: ['question-id', 'prompt', 'chosen', 'rejected']\n",
      "Train len: 22941, Test len: 2549\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8017be95a3164f169a8fa4f5b3b3fd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb99b4663e5476381278cab2e54e9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f125d479454ab4a8041295be9fb553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858fd3edf34d40c2b4144e4722a54066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547376c1107e4701acc07ec47398c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/526 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ZSvedic/gpt4o-arena-brevity-dpo/commit/05370ec13657037acda3a24ba66922076e40f367', commit_message='Upload dataset', commit_description='', oid='05370ec13657037acda3a24ba66922076e40f367', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load JSONL, filter it, rename columns, and push to HuggingFace.\n",
    "\n",
    "# Load the JSONL file.\n",
    "ds = datasets.load_dataset('json', data_files='DATA/dataset_arena_gpt-4o.jsonl')['train']\n",
    "print(f\"Original dataset len: {len(ds)}\")\n",
    "\n",
    "# Filter out duplicates.\n",
    "def filter_out_duplicates(example, seen_questions = set()):\n",
    "    ''' Filter out duplicates. '''\n",
    "    if example['question'] in seen_questions:\n",
    "        return False\n",
    "    else:\n",
    "        seen_questions.add(example['question'])\n",
    "        return True\n",
    "\n",
    "ds = ds.filter(filter_out_duplicates)\n",
    "print(f\"Len after removing duplicate questions: {len(ds)}\")\n",
    "\n",
    "# Filter out bad data.\n",
    "def filter_out_bad_data(example):\n",
    "    ''' Filter out errors and brief answers that are not much shorter than normal answers. '''\n",
    "    brief, normal = example['brief-answer'], example['normal-answer']\n",
    "    # Include only if:\n",
    "    return len(brief)>=1 and len(brief)<len(normal)/2\n",
    "\n",
    "ds = ds.filter(filter_out_bad_data)\n",
    "print(f\"Len after removing bad data: {len(ds)}\")\n",
    "\n",
    "# Rename columns to match DPO format:\n",
    "# https://huggingface.co/docs/trl/dpo_trainer#expected-dataset-format\n",
    "ds = ds.map(lambda row: {\n",
    "    # 'question-id': row['question-id'],\n",
    "    'prompt': row['question'],\n",
    "    'chosen': row['brief-answer'],\n",
    "    'rejected': row['normal-answer'],\n",
    "    })\n",
    "ds = ds.select_columns(['question-id', 'prompt', 'chosen', 'rejected'])\n",
    "print(f\"Columns after renaming: {ds.column_names}\")\n",
    "\n",
    "# Split the dataset into train and test.\n",
    "splits = ds.train_test_split(test_size=0.1)\n",
    "split_ds = datasets.DatasetDict({'train': splits['train'], 'test': splits['test']})\n",
    "print(f\"Train len: {len(split_ds['train'])}, Test len: {len(split_ds['test'])}\")\n",
    "\n",
    "# Upload the dataset to HuggingFace.\n",
    "split_ds.push_to_hub(\"ZSvedic/gpt4o-arena-brevity-dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CELL ###\n",
    "\n",
    "print(f\"Total price: {total_price:.2f} USD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
