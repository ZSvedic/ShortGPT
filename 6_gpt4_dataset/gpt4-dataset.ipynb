{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "import os\n",
    "import huggingface_hub\n",
    "import datasets\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import oaib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants:\n",
    "\n",
    "# Dataset related.\n",
    "ds_conf = SimpleNamespace(\n",
    "    name = \"lmsys/chatbot_arena_conversations\",\n",
    "    skip_rows = 0,\n",
    "    rows_to_process = None, # None for all rows.\n",
    ")\n",
    "\n",
    "# Model Related. Prices: https://openai.com/api/pricing/\n",
    "model_conf = SimpleNamespace(\n",
    "    name = \"gpt-4o\", \n",
    "    in_tok_price = 1.25/1e6, \n",
    "    out_tok_price = 10.00/1e6,\n",
    ")\n",
    "# model_conf = SimpleNamespace(\n",
    "#     name = \"gpt-4o-mini\",\n",
    "#     in_tok_price = 0.15/1e6,\n",
    "#     out_tok_price = 0.60/1e6,\n",
    "# )\n",
    "\n",
    "# Output file related.\n",
    "out_file_name = f\"DATA/dataset_arena_{model_conf.name}.jsonl\"\n",
    "cache_file_name = f\"DATA/dataset_arena_{model_conf.name}.arrow\"\n",
    "\n",
    "# Prompt related.\n",
    "class LimitedPrompt:\n",
    "    def __init__(self, max_words):\n",
    "        self.max_words = max_words\n",
    "        # A word is \"between 5 and 6.5 characters per word including spaces and punctuation\":\n",
    "        # https://charactercounter.com/characters-to-words\n",
    "        self.max_ch_soft = max_words * 6\n",
    "        # Hard limit adds 20% buffer and divides by 4 to get LLM token limit.\n",
    "        # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "        self.max_tokens_hard = int(self.max_ch_soft * 1.2 / 4)\n",
    "\n",
    "normal_prompt = LimitedPrompt(120)\n",
    "normal_prompt.text = f'''Answer the user prompt below \"---\" line. Never exceed {normal_prompt.max_ch_soft} characters / {normal_prompt.max_words} words.\n",
    "---\n",
    "{{question}}\n",
    "'''\n",
    "\n",
    "brief_prompt = LimitedPrompt(20)\n",
    "brief_prompt.text = f'''Given the user prompt and a normal answer, generate a brief answer. A brief answer should be as short as possible but still answer the question and give relevant information. Never exceed {brief_prompt.max_ch_soft} characters / {brief_prompt.max_words} words.\n",
    "Examples between --- lines:\n",
    "--- Example 1 ---\n",
    "Your input:\n",
    "Question: How much is 2+3?\n",
    "Normal answer: Expression 2+3 is equal to 5.\n",
    "Your output:\n",
    "5\n",
    "--- Example 2 ---\n",
    "Your input:\n",
    "Question: What is the color of the sky?\n",
    "Normal answer: The sky is blue.\n",
    "Your output:\n",
    "Blue\n",
    "--- End of examples\n",
    "\n",
    "Considering all the above, give a brief answer to the prompt and normal answer below:\n",
    "Question: {{question}}\n",
    "Normal answer: {{normal_asnwer}}\n",
    "Your output: \n",
    "'''\n",
    "\n",
    "# Run related.\n",
    "batch_size = 500\n",
    "total_price = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace.\n",
    "\n",
    "huggingface_hub.login(token=os.getenv(\"HF_KEY\"))\n",
    "\n",
    "ds = datasets.load_dataset(ds_conf.name)['train']\n",
    "\n",
    "if ds_conf.rows_to_process==None:\n",
    "    ds_conf.rows_to_process=len(ds)-ds_conf.skip_rows\n",
    "\n",
    "ds_range = range(ds_conf.skip_rows, ds_conf.skip_rows+ds_conf.rows_to_process)\n",
    "ds = ds.select(ds_range) \n",
    "print(f\"Loaded dataset range: {ds_range}\")\n",
    "\n",
    "ds = ds.select_columns(['question_id', 'conversation_a'])\\\n",
    "    .rename_column('question_id', 'question-id')\\\n",
    "    .map(lambda example: {'question': example['conversation_a'][0]['content']})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generic batch processing function.\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def batch_call_llm(prompts, max_tokens):\n",
    "    ''' Call the LLM API in a batch using OAIB library. '''\n",
    "\n",
    "    global total_price\n",
    "\n",
    "    # This is very strange. If index is not specified, the API returns the results in a different\n",
    "    # order. If the index is specified, the results are returned in the same order as the input. So,\n",
    "    # we add index but never actually use it.\n",
    "    auto_batch = oaib.Auto(workers=8, index=[\"idx\"])\n",
    "    # auto_batch = oaib.Auto(workers=8)\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        await auto_batch.add(\"chat.completions.create\", \n",
    "                             metadata={\"idx\": idx},\n",
    "                             model=model_conf.name, \n",
    "                             messages=messages, \n",
    "                             max_tokens=max_tokens)                       \n",
    "    \n",
    "    output = await auto_batch.run()\n",
    "    \n",
    "    answers, in_tokens, out_tokens = [], 0, 0\n",
    "    for _, row in output.iterrows():\n",
    "        answers.append(row.result['choices'][0]['message']['content'])\n",
    "        in_tokens += row.result['usage']['prompt_tokens']\n",
    "        out_tokens += row.result['usage']['completion_tokens']\n",
    "    \n",
    "    total_price += in_tokens*model_conf.in_tok_price + out_tokens*model_conf.out_tok_price\n",
    "    \n",
    "    return answers\n",
    "\n",
    "def process_batch_sync(batch, indices, file_handle):\n",
    "    ''' Process a batch of questions synchronously. '''\n",
    "\n",
    "    print(f\"Batch size: {len(indices)}, Start index: {indices[0]}\")\n",
    "\n",
    "    # Run 2-step LLM calls for all questions in the batch.\n",
    "    questions = batch['question']\n",
    "    normal_prompts = [normal_prompt.text.format(question=q) for q in questions]\n",
    "    normal_answers = asyncio.run(batch_call_llm(normal_prompts, normal_prompt.max_tokens_hard))\n",
    "    brief_prompts = [brief_prompt.text.format(question=q, normal_asnwer=na) \n",
    "                     for q, na in zip(questions, normal_answers)]\n",
    "    brief_answers = asyncio.run(batch_call_llm(brief_prompts, brief_prompt.max_tokens_hard))\n",
    "\n",
    "    # Append the batch to JSONL file.\n",
    "    for id, q, np, na, bp, ba in zip(batch['question-id'], questions, \n",
    "                                     normal_prompts, normal_answers, \n",
    "                                     brief_prompts, brief_answers):\n",
    "            entry = {\n",
    "                'question-id': id, 'question': q,\n",
    "                'normal-prompt': np, 'normal-answer': na,\n",
    "                'brief-prompt': bp, 'brief-answer': ba,\n",
    "            }\n",
    "            json.dump(entry, file_handle, ensure_ascii=False)\n",
    "            file_handle.write('\\n')\n",
    "    \n",
    "    # Return new columns.\n",
    "    return {\n",
    "        'normal-prompt': normal_prompts,\n",
    "        'normal-answer': normal_answers,\n",
    "        'brief-prompt': brief_prompts,\n",
    "        'brief-answer': brief_answers,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate both normal and brief answers in batches.\n",
    "\n",
    "with open(out_file_name, 'a') as f:\n",
    "    ds = ds.map(process_batch_sync, batched=True, batch_size=batch_size, with_indices=True,\n",
    "                cache_file_name=cache_file_name, fn_kwargs={'file_handle': f})\n",
    "\n",
    "pprint(ds[4]) # Print a sample row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUG CELL ###\n",
    "\n",
    "# Access and print the cache file paths\n",
    "for cache_file in ds.cache_files:\n",
    "    print(cache_file['filename'])\n",
    "\n",
    "for i in range(11, 15):\n",
    "    print(f\"Q: {ds[i]['question']}\")\n",
    "    print(f\"NORMAL: {ds[i]['normal-answer']}\") \n",
    "    print(f\"BRIEF: {ds[i]['brief-answer']}\") \n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "print(f\"Total price: {total_price:.2f} USD\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-shortgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
